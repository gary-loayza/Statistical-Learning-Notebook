{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "1. [Simple Linear Regression](#Simple-Linear-Regression)\n",
    "2. [Multiple Linear Regression](#Multiple-Linear-Regression)\n",
    "3. [Other Considerations in the Regression Model](#Other-Considerations-in-the-Regression-Model)\n",
    "4. [Comparison of Linear Regression with K-Nearest Neighbors](#Comparison-of-Linear-Regression-with-K-Nearest-Neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear Regression can be used to answer several questions that would be useful in making some prediction about the data.\n",
    "\n",
    "1. Is there a relationship between advertising budget and sales?\n",
    "2. How strong is the relationship between advertising budget and sales?\n",
    "3. Which media contribute to sales?\n",
    "4. How accurately can we estimate the effect of each medium on sales?\n",
    "5. How accurately can we predict future sales?\n",
    "6. Is the relationship linear?\n",
    "7. Is there synergy among the advertising media?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "\n",
    "We assume that there is approximately a linear relationship between X and Y.\n",
    "\n",
    "\\begin{equation}\\label{3.1}\n",
    "    Y \\approx \\beta_0 + \\beta_1 X,\n",
    "    \\tag{3.1}\n",
    "\\end{equation}\n",
    "    where $\\beta_0$ and $\\beta_1$ are intercept and slope terms in the linear model. \n",
    "\n",
    "Once we have used our training model to produce estimates $\\beta_0$ and $\\beta_1$ for the model coefficients, we can predict future sales on the basis of a particular value of response variable by computing\n",
    "\n",
    "\\begin{equation}\\label{3.2}\n",
    "    \\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x,\n",
    "    \\tag{3.2}\n",
    "\\end{equation}\n",
    "    where $\\hat{y}$ indicates a prediction of $Y$ on the basis of $X = x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Coefficients\n",
    "Our goal is to obtain coefficient estimates $\\beta_0$ and $\\beta_1$ such that the linear model (equation \\ref{3.1}) fits the available data well &mdash; that is so that $y_i \\approx \\hat{\\beta}_0 + \\hat{\\beta}_1x_i$ for $i = 1,...,n$. In other words, we want our resulting line as close as possible to the $n$ data points.\n",
    "\n",
    "The most common approach involes minimizing the *least squares*.\n",
    "\n",
    "Let $e_i = y_i - \\hat{y}_i$ represent the $i^{th}$ *residual*.  \n",
    "This is the difference between the $i^{th}$ observed response value and the $i^{th}$ response value that is predicted by our linear model\n",
    "\n",
    "We define the *residual sum of squares* (RSS) as  \n",
    "&nbsp; $\\text{RSS} = e_{1}^2 + e_{2}^2 + ... + e_{n}^2$,  \n",
    "or equivalently\n",
    "\n",
    "\\begin{equation}\\label{3.3}\n",
    "    \\text{RSS} = (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1x_2)^2 + ... + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1x_n)^2\n",
    "    \\tag{3.3}\n",
    "\\end{equation}\n",
    "\n",
    "The least squares approach chooses $\\beta_0$ and $\\beta_1$ to minimize the \\text{RSS}. Using some calculus, one can show that the minimizers are\n",
    "\n",
    "\\begin{equation}\\label{3.4}\n",
    "    \\hat{\\beta}_1 = \\frac{\\sum^n_{i=1}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum^n_{i=1}(x_i - \\bar{x})^2} ,\\\\\n",
    "    \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x} ,\n",
    "    \\tag{3.4}\n",
    "\\end{equation}\n",
    "where $\\bar{y}$ and $\\bar{x}$ are the sample means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the Accuracy of the Coefficient Estimates\n",
    "If the function for $Y$ is to be approximated by a linear function, then the relationship in (equation 2.1) can be rewritten as:\n",
    "\n",
    "\\begin{equation}\\label{3.5}\n",
    "    Y = \\beta_0 + \\beta_1X + \\epsilon\n",
    "    \\tag{3.5}\n",
    "\\end{equation}\n",
    "\n",
    "This model defines the *population regression line*, which is the best linear approximation to the true relationship between $X$ and $Y$. The least squares regression coefficient estimates (\\ref{3.4}) characterize the *least squares line* (\\ref{3.2}).\n",
    "\n",
    "An analogy can be made between linear regression and estimation of the mean of a random variable on the basis of *bias*.\n",
    "\n",
    "If we use the sample mean μ̂ to estimate μ, this estimate is *unbiased*, in the sense on average, we expect μ̂ to equal μ. μ̂ might overestimate μ on the basis of one particular data set, and underestimate μ on another set of observations. But you could imagine that on an increasingly large set of observations, then this average would *exactly* equal μ.\n",
    "\n",
    "So how accurate is the sample mean μ̂ as an estimate of μ? In general we answer this question by computing the *standard error* of μ̂ written as $SE(\\hat{\\mu})$.\n",
    "\n",
    "\\begin{equation}\\label{3.7}\n",
    "    Var(\\hat{\\mu}) = SE(\\hat{\\mu})^2 = \\frac{\\sigma^2}{n}\n",
    "    \\tag{3.7}\n",
    "\\end{equation}\n",
    "where $\\sigma$ is the standard deviation of each of the realizations of $y_i$ of $Y$.\n",
    "\n",
    "In a similar vein, we can wonder how close $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are to the true values $\\beta_0$ and $\\beta_1$. To compute the standard errors associated with $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, we use the following formulas:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\label{3.8}\n",
    "    SE(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\right] ,\n",
    "    \\; SE(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2}\n",
    "    \\tag{3.8}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "where $\\sigma^2 = Var(\\epsilon)$\n",
    "\n",
    "In general, $\\sigma^2$ is not known, but can be estimated from the data. The estimate of $\\sigma$ is known as the *residual standard error*, and is given by the formula $\\text{RSE} = \\sqrt{\\text{RSS} / (n - 2)}$\n",
    "\n",
    "Standard errors can be used to compute *confidence intervals*. For linear regression, the 95% confidence interval for $\\beta_1$ approximately takes the form\n",
    "\n",
    "\\begin{equation}\\label{3.9}\n",
    "    \\beta_1 \\pm 2 \\cdot SE(\\hat{\\beta}_1)\n",
    "    \\tag{3.9}\n",
    "\\end{equation}\n",
    "\n",
    "That is, there is approximately a 95% chance that the interval\n",
    "\n",
    "\\begin{equation}\\label{3.10}\n",
    "    \\left[ \\beta_1 - 2 \\cdot SE(\\hat{\\beta}_1) , \\beta_1 + 2 \\cdot SE(\\hat{\\beta}_1) \\right]\n",
    "    \\tag{3.10}\n",
    "\\end{equation}\n",
    "\n",
    "will contain the true value of $\\beta_1$. Similarly, $\\beta_0$\n",
    "\n",
    "\\begin{equation}\\label{3.11}\n",
    "    \\beta_0 \\pm 2 \\cdot SE(\\hat{\\beta}_0)\n",
    "    \\tag{3.11}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard errors can also be used to perform *hypothesis test* on the coefficients.\n",
    "\n",
    "The Null Hypothesis:\n",
    "\\begin{equation}\\label{3.12}\n",
    "        H_0 : \\text{There is no relationship between X and Y}\n",
    "        \\tag{3.12}\n",
    "\\end{equation}\n",
    "\n",
    "vs\n",
    "\n",
    "The Alternative Hypothesis:\n",
    "\\begin{equation}\\label{3.13}\n",
    "    H_a: \\text{There is relationship between X and Y.}\n",
    "    \\tag{3.13}\n",
    "\\end{equation}\n",
    "\n",
    "Mathematically, this corresponds to testing\n",
    "\n",
    "&nbsp;&nbsp; $H_0 : \\beta_1 = 0$ &nbsp; versus &nbsp; $H_a : \\beta_1 \\neq 0,$\n",
    "\n",
    "since if $\\beta_1 = 0$ then model \\ref{3.5} reduces to $Y = \\beta_0 + \\epsilon$, and $X$ is not associated with $Y$. To test the null hypothesis, we need to determine whether $\\hat{\\beta}_1$, our estimate for $\\beta_1$, is sufficiently far from  zero that we can be confident that $\\beta_1$ is non-zero. So if $SE(\\hat{\\beta}_1)$ is small, then even relatively small values of $\\hat{\\beta}_1$ may provide strong evidence that $\\beta_1 \\neq 0$\n",
    "\n",
    "In practice, we compute a *t-statistic*, given by\n",
    "\n",
    "\\begin{equation}\\label{3.14}\n",
    "    t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)},\n",
    "    \\tag{3.14}\n",
    "\\end{equation}\n",
    "which measures the number of standard deviations that $\\hat{\\beta}_1$ is away from 0.\n",
    "\n",
    "If there really is no relationship between $X$ and $Y$, then we expect that equation (\\ref{3.14}) will have a t-distribution with $n − 2$ degrees of freedom. The t-distribution has a bell shape and for values of $n > 30$ it is quite similar to the normal distribution. Consequently, it is a simple matter to compute the probability of observing any number equal to $| t |$ or larger in absolute value, assuming $\\beta_1 = 0$. We call this probability the *p-value*.\n",
    "\n",
    "A small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance.Hence, if we see a small *p-value*, then we can *reject the Null Hypothesis*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the Accuracy of the Model\n",
    "The quality of a linear regression fit is typically assessed by using two related quantities: The residual standard error $(RSE)$ and the $R^2$ statistic.\n",
    "\n",
    "### Residual Standard Error\n",
    "Recall from the model (\\ref{3.5}) that associated with each observation is an error term $\\epsilon$. The $RSE$ is an estimate of the standard deviation of $\\epsilon$. Roughly speaking, it is the average amount that the response will deviate from the true regression line.\n",
    "\n",
    "\\begin{equation}\\label{3.15}\n",
    "    RSE = \\sqrt{ \\frac{1}{n - 2} RSS } = \\sqrt{ \\frac{1}{n - 2} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 }\n",
    "    \\tag{3.15}\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\\label{3.16}\n",
    "    RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 .\n",
    "    \\tag{3.16}\n",
    "\\end{equation}\n",
    "\n",
    "The $RSE$ is considered a measure of the lack of fit of the model (\\ref{3.5}) to the data. If $\\hat{y}_i \\approx y_i$ for $i = 1, ... , n$ then equation (\\ref{3.15}) will be small, and we can conclude that the model fits the data very well.\n",
    "\n",
    "### $R^2$ Statistic\n",
    "Since $RSE$ is measured in the units of $Y$, it is not always clear what constitutes a good value of $RSE$. $R^2$ offers an alternative, it takes the proportion of variance explained, and so always takes a value between 0 and 1.\n",
    "\n",
    "To calculate $R^2$,\n",
    "\n",
    "\\begin{equation}\\label{3.17}\n",
    "    R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\n",
    "    \\tag{3.17}\n",
    "\\end{equation}\n",
    "\n",
    "where $TSS = \\sum(y_i - \\bar{y})^2$ is the *total sum of squares*.\n",
    "*TSS* can be thought of as the amount of variability inherent in the response with respect to the sample average. In contrast, $RSS$ (\\ref{3.16}) measures the amount of variability that is left unexplained after performing the regression.\n",
    "\n",
    "Hence, $TSS - RSS$ measures **the amount of variability in the response that is explained (or removed) by performing the regression**, and $R^2$ measures **the proportion of variability in $Y$ that can be explained using $X$**.\n",
    "\n",
    "An $R^2$ Statistic that is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression. A number near 0 indicates that the regression did not explain much of the variability in the response; this might occur because the linear model is wrong, or the inherent error $\\sigma^2$ is high, or both.\n",
    "\n",
    "However, it can still be challenging to determine what is a *good* $R^2$ value. In general, this will depend on the application. In cases where we *know* that the data comes from a linear model with small residual error, we would expect to see $R^2$ extremely close to 1, and a substantially smaller $R^2$ might indicate a problem with the experimentation.\n",
    "\n",
    "In rougher approximations of the data with the linear model, the residual errors due to the other unmeasured factors are often very large. In this setting, we would  expect only a very small proportion of the variance in the response to be explained by the predictor.  \n",
    "Then an $R^2$ value well below 0.1 might be more realistic.\n",
    "\n",
    "The $R^2$ statistic is a measure of linear relationship between $X$ and $Y$. Recall that correlation, defined as\n",
    "\n",
    "\\begin{equation}\\label{3.18}\n",
    "    \\text{Cor }(X,Y) = \\frac{ \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }{ \\sqrt{ \\sum_{i=1}^n (x_i - \\bar{x})^2 } \\sqrt{ \\sum_{i=1}^n (y_i - \\bar{y})^2} } ,\n",
    "    \\tag{3.18}\n",
    "\\end{equation}\n",
    "\n",
    "is also a good measure of the linear relationship between $X$ and $Y$. This suggests that we might be able to use $r = \\text{Cor }(X, Y )$ instead of $R^2$ in order to assess the fit of the linear model. In fact, it can be shown that in the simple linear regression setting, $R^2 = r^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "Simple linear regression is a useful approach for predicting a response on the basis of a single predictor variable. However, in practice we often have more than one predictor. How can we accommodate any additional predictors?\n",
    "\n",
    "One option is to run three separate simple linear regressions, each of which uses a different advertising medium as a predictor. However, it is unclear how to make a single prediction based on some given combination of all variables which may all or in part be correlated.Instead of fitting a separate simple linear regression model for each predictor, a better approach is to extend the simple linear regression model (\\ref{3.5}) so that it can directly accommodate multiple predictors. We can do this by giving each predictor a separate slope coefficient in a single model.\n",
    "\n",
    "In general, suppose that we have *p* distinct predictors. Then the multiple linear regression model takes the form\n",
    "\n",
    "\\begin{equation}\\label{3.19}\n",
    "    Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon\n",
    "    \\tag{3.19}\n",
    "\\end{equation}\n",
    "\n",
    "where $X_j$ represents the $j^{th}$ predictor and $\\beta_j$ quantifies the association between that variable and the response.\n",
    "\n",
    "We interpret $\\beta_j$ as the *average* effect on $Y$ of a one unit increase in $X_j$, *holding all other predictors fixed*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Regression Coefficients\n",
    "As was the case in the simple linear regression setting, the regression coefficients $\\beta_0, \\beta_1 , ... , \\beta_p$ in (\\ref{3.19}) are unknown, and must be estimated. Given estimates $\\hat{\\beta}_0 , \\hat{\\beta}_1 , ... , \\hat{\\beta}_p$ , we can make predictions using the formula\n",
    "\n",
    "\\begin{equation}\\label{3.21}\n",
    "    \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + ... + \\hat{\\beta}_p x_p\n",
    "    \\tag{3.12}\n",
    "\\end{equation}\n",
    "\n",
    "The parameters are estimated using the same least squares approach that we saw in the context of simple linear regression. We choose $\\beta_0 , \\beta_1 , ... , \\beta_p$ to minimize the sum of squared residuals.\n",
    "\n",
    "\\begin{equation}\\label{3.22}\n",
    "    RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\n",
    "        = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2} - ... - \\hat{\\beta}_p x_{ip} )^2 .\n",
    "        \\tag{3.22}\n",
    "\\end{equation}\n",
    "\n",
    "The values $\\hat{\\beta}_0, \\hat{\\beta}_1,...,\\hat{\\beta}_p$ that minimize (\\ref{3.22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Important Questions\n",
    "When we perform multiple linear regression, we usually are interested in\n",
    "answering a few important questions.\n",
    "\n",
    "1. Is at least one of the predictors $X_1, X_2, . . . , X_p$ useful in predicting the response?\n",
    "2. Do all the predictors help to explain $Y$, or is only a subset of the predictors useful?\n",
    "3. How well does the model fit the data?\n",
    "4. Given a set of predictor values, what response value should we predict, and how accurate is our prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One: Is There a Relationship Between the Response and Predictors?\n",
    "Recall that in the simple linear regression setting, in order to determine\n",
    "whether there is a relationship between the response and the predictor we\n",
    "can simply check whether $\\beta_1 = 0$.\n",
    "\n",
    "In the multiple regression setting with $p$ predictors, we need to ask whether all of the regression coefficients are zero. We test the Null Hypothesis,\n",
    "\n",
    "\\begin{align*}\n",
    "    H_0: \\beta_1 = \\beta_2 = ... = \\beta_p = 0\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "versus the alternative\n",
    "\n",
    "\\begin{align*}\n",
    "    H_a: \\text{at least one } \\beta_j \\text{ is non-zero}\n",
    "\\end{align*}\n",
    "\n",
    "This hypothesis test is performed by computing the *F-statistic*,\n",
    "\n",
    "\\begin{equation}\\label{3.23}\n",
    "    F = \\frac{(\\text{TSS} - \\text{RSS}) / p}{\\text{RSS}/(n-p-1)},\n",
    "    \\tag{3.23}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\text{TSS} = \\sum(y_i - \\bar{y})^2$ and $\\text{RSS} = \\sum(y_i - \\hat{y}_i)^2$. If the linear model assumptions are correct, one can show that\n",
    "\n",
    "\\begin{align*}\n",
    "    E\\{\\text{RSS}/(n − p − 1)\\} = \\sigma^2\n",
    "\\end{align*}\n",
    "\n",
    "and that, provided $H_0$ is true,\n",
    "\n",
    "\\begin{align*}\n",
    "    E\\{(\\text{TSS} − \\text{RSS})/p \\} = \\sigma^2.\n",
    "\\end{align*}\n",
    "\n",
    "Hence, when there is no relationship between the response and predictors, one would expect the *F-statistic* to take on a value close to 1. On the other hand, if $H_a$ is true, then $\\;E\\{(\\text{TSS} − \\text{RSS})/p \\} > \\sigma^2$, so we expect $F$ to be greater than 1.\n",
    "\n",
    "How large does the *F-statistic* need to be before we can reject $H_0$ and conclude that there is a relationship? It turns out that the answer depends on the values of $n$ and $p$. **When $n$ is large,** an F-statistic that is just a little larger than 1 might still provide evidence against $H_0$. In contrast, **a larger F-statistic is needed to reject $H_0$ if $n$ is small.**\n",
    "\n",
    "When $H_0$ is true, the F-statistic generally follows an F-distribution. Although this is mostly true for normally distributed errors $\\epsilon_i$, it is also true for not normally-distributed $\\epsilon_i$ given a large sample size $n$.\n",
    "\n",
    "For any value of $n$ and $p$, any statistical software can be used to compute the p-value associated with the F-statistic. With a low p-value from the software computation, we can reasonably expect that at least one of the predictors is associated with the response.\n",
    "\n",
    "In (\\ref{3.23}) we are testing $H_0$ that all coefficients are zero. Sometimes we want to test that a subset $q$ of the coefficients are zero. This corresponds to a null hypothesis of the form\n",
    "\n",
    "\\begin{align*}\n",
    "    H_0: \\beta_{p-q+1} = \\beta_{p-q+2} = ... = \\beta_{p} = 0,\n",
    "\\end{align*}\n",
    "\n",
    "where for convenience we have put the variables chosen for omission at the end of the list. In this case, we fit a second model that uses all the variables *except* those last $q$.\n",
    "\n",
    "Suppose  that the residual sum of squares for that model is $\\text{RSS}_0$. Then the appropriate F-statistic is\n",
    "\n",
    "\\begin{equation}\\label{3.24}\n",
    "    F = \\frac{( \\text{RSS}_0 - \\text{RSS} )/q}{ \\text{RSS}/ (n-p-1) } .\n",
    "    \\tag{3.24}\n",
    "\\end{equation}\n",
    "\n",
    "where $q$ is the number of those predictors that we wish to exclude.\n",
    "\n",
    "Given these individual p-values for each variable, why do we need to look\n",
    "at the overall F-statistic? After all, it seems likely that if any one of the\n",
    "p-values for the individual variables is very small, then **at least one of the\n",
    "predictors is related to the response.** However, this logic is flawed, especially\n",
    "when the number of predictors $p$ is large.\n",
    "\n",
    "For instance, consider an example in which $p = 100$ and $H_0 : β_1 = β_2 = . . . = β_p = 0$ is true, so no variable is truly associated with the response. In this situation, about $5\\%$ of the *individual* p-values associated with each variable will be below 0.05 by chance.\n",
    "\n",
    "Hence, if we use the individual t-statistics and associated p-values there is a very high chance that we will\n",
    "incorrectly conclude that there is a relationship. However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors. Hence, if $H_0$ is true, there is only a $5\\%$ chance that the F-statistic will result in a p-value below 0.05, regardless of the number of predictors or the number of observations.\n",
    "\n",
    "The approach of using an F-statistic to test for any association between the predictors and the response works when $p$ is relatively small compared to $n$. However, sometimes we have a very large number of variables. If $p > n$ then there are more coefficients $β_j$ to estimate than observations from which to estimate them.\n",
    "\n",
    "In this case we cannot even fit the multiple linear regression model using least squares, so the F-statistic cannot be used, and neither can most of the other concepts that we have seen so far.\n",
    "\n",
    "When $p$ is large, some of the approaches discussed in the next section, such as forward selection, can be used. This *high-dimensional* setting is discussed in greater detail in Chapter 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two: Deciding on Important Variables\n",
    "If we conclude on the basis of that p-value that at least one of the predictors is related to the response, then it is natural to wonder *which* are the guilty ones!\n",
    "\n",
    "The task of determining which predictors are associated with the response, in order to fit a single model involving only those predictors, is referred to as *variable selection. (For more, see Chapter 6)*\n",
    "\n",
    "Ideally, we would like to perform variable selection by trying out a lot of different models, each containing a different subset of the predictors. For instance, if $p = 2$, then we can consider four models: (1) a model containing no variables, (2) a model containing $X_1$ only, (3) a model containing $X_2$ only, and (4) a model containing both $X_1$ and $X_2$.\n",
    "\n",
    "Then we determine which is best using various statistics, including *Mallow’s* $C_p$, *Akaike information criterion* (AIC), *Bayesian information criterion* (BIC), and adjusted $R_2$.\n",
    "\n",
    "Unfortunately, there are a total of $2^p$ models that contain subsets of $p$ variables. Therefore, unless $p$ is very small, we cannot consider all $2^p$ models, and instead we need an automated and efficient approach to choose a smaller set of models to consider.\n",
    "\n",
    "There are **three** classical approaches for this task:\n",
    "\n",
    "- **Forward selection.** We begin with the *null model*&mdash;a model that contains an intercept but no predictors. We then fit $p$ simple linear regressions and add to the null model the variable that results in the lowest RSS. We then add to that model the variable that results in the lowest RSS for the new two-variable model. This approach is continued until some stopping rule is satisfied.\n",
    "\n",
    "\n",
    "- **Backward selection.** We start with all variables in the model, andremove the variable with the largest p-value&mdash;that is, the variable that is the least statistically significant. The new (p − 1)-variable model is fit, and the variable with the largest p-value is removed. This procedure continues until a stopping rule is reached. For instance, we may stop when all remaining variables have a p-value below some threshold.\n",
    "\n",
    "\n",
    "- **Mixed selection.** We start with no variables in the model, and add the variable that provides the best fit, one-by-one. If at any point the p-value for one of the variables in the model rises above a certain threshold, then we remove that variable from the model. We continue to perform these forward and backward steps until all variables in the model have a sufficiently low p-value, and all variables outside the model would have a large p-value if added to the model.\n",
    "\n",
    "*Note*: Backward selection cannot be used if $p > n$, while forward selection can always be used. Forward selection is a greedy approach, and might include variables early that later become redundant. Mixed selection can remedy this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three: How Well Does the Model Fit the Data?\n",
    "Two of the most common numerical measures of model fit are the $\\text{RSE}$ and $R^2$, the fraction of variance explained.\n",
    "\n",
    "Recall that in simple regression, $R^2$ is the square of the correlation of the response and the variable. In multiple linear regression, it turns out that it equals $\\text{Cor }(Y, \\hat{Y})^2$, the square of the correlation between the response and the fitted linear model.\n",
    "\n",
    "It turns out that $R^2$ will always increase when more variables are added to the model, even if those variables are only weakly associated with the response. This is due to the fact that adding another variable to the least squares equations must allow us to fit the training data (though not necessarily the testing data) more accurately. Essentially, variables that provide *only small increases* to $R^2$ should be omitted or we risk overfitting the model to our training data.\n",
    "\n",
    "In these cases it may be worth analysing $\\text{RSE}$ in general form\n",
    "\n",
    "\\begin{equation}\\label{3.25}\n",
    "    \\text{RSE} = \\sqrt{ \\frac{1}{n - p - 1} \\text{RSS} }\n",
    "    \\tag{3.25}\n",
    "\\end{equation}\n",
    "\n",
    "which you may notice simplifies to (\\ref{3.15}) for a simple linear regression ($p = 1$). Thus, models with more variables can have higher $\\text{RSE}$ if the decrease in $\\text{RSS}$ is small relative to the increase in $p$.\n",
    "\n",
    "Additionally, plotting the data can provide insights on how to imporve the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four: Predictions\n",
    "Once we have fit the multiple regression model, it is straightforward to\n",
    "apply (\\ref{3.21}) in order to predict the response $Y$ on the basis of a set of\n",
    "values for the predictors $X_1 , X_2 , . . . , X_p$. However, there are *three* sorts of\n",
    "uncertainty associated with this prediction.\n",
    "\n",
    "1. The *least squares plane* $\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + ... + \\hat{\\beta}_p X_p $ is only an estimate for the *true population regression plane* $f(X) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p.$ The inaccuracy in the coefficient estimates is related to the *reducible error* from Chapter 2. We can compute a *confidence interval* in order to determine how close $\\hat{Y}$ will be to $f(X)$.\n",
    "\n",
    "\n",
    "2. In practice assuming a linear model for $f(X)$ is almost always an approximation of reality, so there is an additional source of potentially reducible error which we call *model bias*.\n",
    "\n",
    "\n",
    "3. Even if we knew $f(X)$—that is, even if we knew the true values for $β_0 , β_1 , . . . , β_p$—the response value cannot be predicted perfectly because of the random error $\\epsilon$ in the model (\\ref{3.21}). How much will $Y$ vary from $\\hat{Y}$? We use *prediction intervals* to answer this question. Prediction intervals are always wider than confidence intervals, because they incorporate both the error in the estimate for $f(X)$ (the reducible error) and the uncertainty as to how much an individual point will differ from the population regression plane (the irreducible error).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Considerations in the Regression Model\n",
    "## Qualitative Predictors\n",
    "\n",
    "In our discussion so far, we have assumed that all variables in our linear regression model are *quantitative*. But often some predictors are *qualitative*.\n",
    "\n",
    "|                | Coefficient | Std. error | t-statistic | p-value  |\n",
    "|----------------|-------------|------------|-------------|----------|\n",
    "| **Intercept**      | 509.80      | 33.13      | 15.389      | < 0.0001 |\n",
    "| **gender[Female]** | 19.73       | 46.05      | 0.429       | 0.6690   |\n",
    "\n",
    "> Least squares coefficient estimates associated with\n",
    "> the regression of balance onto gender in the Credit data set.\n",
    "> The average credit card debt for males is estimated to be $ \\$509.80 $,\n",
    "> whereas females are estimated to carry $\\$19.73$ in additional debt for a total of $ \\$509.80 + \\$19.73 = \\$529.53 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors with Only Two Levels\n",
    "Suppose that we wish to investigate differences in credit card balance between males and females.\n",
    "If a qualitative predictor (also known as a factor) only has two levels, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or *dummy variable* that takes on two possible numerical values. For example, based on the gender variable, we can create variable a new variable that takes the form\n",
    "\n",
    "\\begin{equation}\\label{3.26}\n",
    "    x_i = \\begin{cases} 1 & \\text{ if } i \\text{th person is female,} \\\\ 0 & \\text{ if } i \\text{th person is male} \\end{cases}\n",
    "    \\tag{3.26}\n",
    "\\end{equation}\n",
    "\n",
    "and use this variable as a predictor in the regression equation. This results in the model\n",
    "\n",
    "\\begin{equation}\\label{3.27}\n",
    "    y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i = \\begin{cases} \\beta_0 + \\beta_1 + \\epsilon_i & \\text{ if } i \\text{th person is female,} \\\\ \\beta_0 + \\epsilon_i & \\text{ if } i \\text{th person is male} \\end{cases}\n",
    "    \\tag{3.27}\n",
    "\\end{equation}\n",
    "\n",
    "Now $\\beta_0$ can be interpreted as the average credit card balance among males, $β_0 + β_1$ as the average among females, and $β_1$ as the average difference in credit card balance between females and males.\n",
    "\n",
    "However, we notice that the p-value for the dummy variable is very high. This indicates that there is no statistical evidence of a difference in average credit card balance between the genders.\n",
    "\n",
    "The decision to code females as 1 and males as 0 is arbitrary, and has no effect on the regression fit, but does alter the interpretation of the coefficients. If we had coded males as 1 and females as 0, then the estimates\n",
    "for $β_0$ and $β_1$ would have been 529.53 and −19.73, respectively. Alternatively, instead of a $0/1$ coding scheme, we could create a dummy variable\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    x_i = \\begin{cases} 1 & \\text{ if } i \\text{th person is female,} \\\\ -1 & \\text{ if } i \\text{th person is male} \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "and use this variable in the regression equation. This results in the model\n",
    "\n",
    "\\begin{align*}\n",
    "    y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i = \\begin{cases} \\beta_0 + \\beta_1 + \\epsilon_i & \\text{ if } i \\text{th person is female,} \\\\ \\beta_0 - \\beta_1 + \\epsilon_i & \\text{ if } i \\text{th person is male} \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Now $β_0$ can be interpreted as the overall average credit card balance (ignoring the gender effect), and $β_1$ is the amount that females are above the average and males are below the average. In this example, the estimate for $β_0$ would be $\\$519.665$, halfway between the male and female averages of $\\$509.80$ and $\\$529.53$. The estimate for $β_1$ would be $\\$9.865$, which is half of $\\$19.73$, the average difference between females and males.\n",
    "\n",
    "It is important to note that the final predictions for the credit balances of males and females will be identical regardless of the coding scheme used. The only difference is in the way that the coefficients are interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Predictors with More than Two Levels\n",
    "When a qualitative predictor has more than two levels, a single dummy\n",
    "variable cannot represent all possible values. In this situation, we can create\n",
    "additional dummy variables. For example, for the ethnicity variable we\n",
    "create two dummy variables. The first could be\n",
    "\n",
    "\\begin{equation}\\label{3.28}\n",
    "    x_{i1} = \\begin{cases} 1 & \\text{ if } i \\text{th person is Asian,} \\\\ 0 & \\text{ if } i \\text{th person is not Asian} \\end{cases}\n",
    "    \\tag{3.28}\n",
    "\\end{equation}\n",
    "\n",
    "and the second could be\n",
    "\n",
    "\\begin{equation}\\label{3.29}\n",
    "    x_{i2} = \\begin{cases} 1 & \\text{ if } i \\text{th person is Caucasian,} \\\\ 0 & \\text{ if } i \\text{th person is not Caucasian} \\end{cases}\n",
    "    \\tag{3.29}\n",
    "\\end{equation}\n",
    "\n",
    "Then both of these variables can be used in the regression equation, in order to obtain the model\n",
    "\n",
    "\\begin{equation}\\label{3.30}\n",
    "    y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\epsilon_i = \\begin{cases} \\beta_0 + \\beta_1 + \\epsilon_i & \\text{ if } i \\text{th person is Asian,} \\\\ \\beta_0 + \\beta_2 + \\epsilon_i & \\text{ if } i \\text{th person is Caucasian} \\\\ \\beta_0 + \\epsilon_i & \\text{ if } i \\text{th person is African American}\\end{cases}\n",
    "    \\tag{3.30}\n",
    "\\end{equation}\n",
    "\n",
    "Now $β_0$ can be interpreted as the average credit card balance for African\n",
    "Americans, $β_1$ can be interpreted as the difference in the average balance\n",
    "between the Asian and African American categories, and $β_2$ can be interpreted as the difference in the average balance between the Caucasian and African American categories. There will always be one fewer dummy variable than the number of levels. The level with no dummy variable—African American in this example—is known as the *baseline*.\n",
    "\n",
    "\n",
    "|                          | Coefficient | Std. error | t-statistic | p-value  |\n",
    "|--------------------------|-------------|------------|-------------|----------|\n",
    "| **Intercept**            | 531.00      | 46.32      | 11.464      | < 0.0001 |\n",
    "| **ethnicity[Asian]**     | -18.69      | 65.02      | -0.287      | 0.7740   |\n",
    "| **ethnicity[Caucasian]** | -12.50      | 56.68      | -0.221      | 0.8260   |\n",
    "\n",
    "> Least squares coefficient estimates associated with the regression of balance onto ethnicity in\n",
    "> the Credit data set.\n",
    "\n",
    "We see that the estimated balance for the baseline, African American, is $\\$531.00$. It is estimated that the\n",
    "Asian category will have $\\$18.69$ less debt than the African American category, and that the Caucasian category\n",
    "will have $\\$12.50$ less debt than the African American category. Again from the table, the p-values associated with the coefficient estimates for the two dummy variables are very large, suggesting no statistical evidence of a real difference in credit card balance between the ethnicities.\n",
    "\n",
    "Rather than rely on the individual coefficients, we can use an F-test to test $H_0 : β_1 = β_2 = 0$. This F-test has a p-value of $0.96$, indicating that we **cannot reject the null hypothesis** that there is no relationship between balance and ethnicity.\n",
    "\n",
    "To regress balance on both a quantitative variable such as income and a qualitative variable such as student, we must simply create a dummy variable for student and then fit a multiple regression model using income and the dummy variable as predictors for credit card balance.\n",
    "\n",
    "There are many different ways of coding qualitative variables besides\n",
    "the dummy variable approach taken here. All of these approaches lead to\n",
    "equivalent model fits, but the coefficients are different and have different\n",
    "interpretations, and are designed to measure particular *contrasts*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions of the Linear Model\n",
    "The standard linear regression model (\\ref{3.19})  makes several highly restrictive assumptions that are often violated in practice. Two of the most important assumptions state that the relationship between the predictors and response are *additive* and *linear*. The **additive assumption** means that changes in a predictor $X_j$ on the response $Y$ is independent of the values of the other predictors. The **linear assumption** states that the change in the response $Y$ due to a one-unit change in $X_j$ is constant.\n",
    "\n",
    "For example, if given a fixed advertising budget of $\\$100,000$, spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or to radio. In marketing, this is known as a\n",
    "*synergy effect*, and in statistics it is referred to as an *interaction effect*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the Additive Assumption\n",
    "One way of extending this model to allow for interaction effects is to include a third predictor, called an interaction term, which is constructed by computing the product of $X_1$ and $X_2$ . This results in the model\n",
    "\n",
    "\\begin{equation}\\label{3.31}\n",
    "    Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1X_2 + \\epsilon\n",
    "    \\tag{3.31}\n",
    "\\end{equation}\n",
    "\n",
    "which now has a term that is dependent on all predictors in the model, which will relax the additive assumption. We can interpret $β_3$ as the increase in the effectiveness of $X_1$ for a one unit increase in $X_2$ (or vice-versa).\n",
    "\n",
    "|               | Coefficient | Std. error | t-statistic | p-value  |\n",
    "|---------------|-------------|------------|-------------|----------|\n",
    "| **Intercept** | 6.7502      | 0.248      | 27.23       | < 0.0001 |\n",
    "| **TV**        | 0.0191      | 0.002      | 12.70       | < 0.0001 |\n",
    "| **radio**     | 0.0289      | 0.009      | 3.24        | 0.0014   |\n",
    "| **TV×radio**  | 0.0011      | 0.000      | 20.73       | < 0.0001 |\n",
    "\n",
    "> For the Advertising data, least squares coefficient estimates associated with\n",
    "> the regression of sales onto TV and radio, with an interaction term.\n",
    "\n",
    "The results in the above table strongly suggest that the model that includes the interaction term is superior to the model that contains only *main effects*. The p-value for the interaction term, $\\text{TV} \\times \\text{radio}$, is extremely low, indicating that there is strong evidence for $H_a: β3 \\neq 0$. In other words, it is clear that\n",
    "the true relationship is not additive.\n",
    "\n",
    "The $R^2$ for the model (3.33) is $96.8 \\%$, compared to only $89.7\\%$ for the model that predicts sales using TV and radio without an interaction term. This means that $(96.8 − 89.7)/(100 − 89.7) = 69 \\%$ of the variability in sales that remains after fitting the additive model has been explained by the interaction term.\n",
    "\n",
    "In this example, the p-values associated with TV, radio, and the interaction term all are statistically significant, and so it is obvious that all three variables should be included in the model. However, it is sometimes the case that an interaction term has a very small p-value, but the associated main effects (in this case, TV and radio) do not. The **hierarchical principle** states that *if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant*.\n",
    "\n",
    "The rationale for this principle is that if $X_1 \\times X_2$ is related to the response, then whether or not the coefficients of $X_1$ or $X_2$ are exactly zero is of little interest. Also $X_1 \\times X_2$ is typically correlated with $X_1$ and $X_2$, and so leaving them out tends to alter the meaning of the interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear Relationships\n",
    "In later chapters, we will present more complex approaches for performing non-linear fits in more general settings.\n",
    "Here we present a very simple way to directly extend the linear model to accommodate non-linear relationships, using **polynomial regression**.\n",
    "\n",
    "A simple approach for incorporating non-linear associations in a linear model is to include transformed versions of the predictors in the model. In the figure below, the orange line represents the linear regression fit. There is a pronounced relationship between *mpg* and *horsepower*, but it seems clear that this relationship is in fact non-linear: the data suggest a curved relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean Data for Plot\n",
    "auto = pd.read_csv('../data/Auto.csv')\n",
    "auto['horsepower'] = auto['horsepower'].apply(lambda x:  np.nan if x == '?' else x )\n",
    "\n",
    "auto['horsepower'] = pd.to_numeric(auto['horsepower'], downcast='float')\n",
    "hp = auto.dropna()\n",
    "\n",
    "# Fit and Plot\n",
    "xp = np.linspace(40, 240, 100)\n",
    "\n",
    "z = np.polyfit(hp['horsepower'], hp['mpg'], 1)\n",
    "z2 = np.polyfit(hp['horsepower'], hp['mpg'], 2)\n",
    "z5 = np.polyfit(hp['horsepower'], hp['mpg'], 5)\n",
    "\n",
    "p = np.poly1d(z)\n",
    "p2 = np.poly1d(z2)\n",
    "p5 = np.poly1d(z5)\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "ax.set_xlabel('Horsepower')\n",
    "ax.set_ylabel('Miles per gallon')\n",
    "\n",
    "ax.plot(hp['horsepower'], hp['mpg'], '.')\n",
    "ax.plot(xp, p(xp), label='Linear', lw=3)\n",
    "ax.plot(xp, p2(xp), label='Degree 2', lw=3)\n",
    "ax.plot(xp, p5(xp), '--', label='Degree 5', lw=3)\n",
    "\n",
    "ax.legend(loc=(0.7,0.6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points in the figure seem to have a quadratic shape, suggesting that a model of the form\n",
    "\n",
    "\\begin{equation}\\label{3.36}\n",
    "    \\text{mpg} = \\beta_0 + \\beta_1 \\times \\text{horsepower} + \\beta_2 \\times \\text{horsepower}^2 + \\epsilon\n",
    "    \\tag{3.36}\n",
    "\\end{equation}\n",
    "\n",
    "may provide a better fit. Equation \\ref{3.36} involves predicting *mpg* using a non-linear function of *horsepower*. **But it is still a linear model!** That is, (\\ref{3.36}) is simply a multiple linear regression model with $X_1 = \\text{horsepower}$ and $X_2 = \\text{horsepower}^2$.\n",
    "\n",
    "The green curve in the figure shows the resulting quadratic fit to the data. The quadratic fit appears to be substantially better than the fit obtained when just the linear term is included. The $R^2$ of the quadratic fit is $0.688$, compared to $0.606$ for the linear fit, and the p-value in the table for the quadratic term is highly significant.\n",
    "\n",
    "|                    | Coefficient | Std. error | t-statistic | p-value  |\n",
    "|--------------------|-------------|------------|-------------|----------|\n",
    "| **Intercept**      | 56.9001     | 1.8004     | 31.6        | < 0.0001 |\n",
    "| **horsepower**     | −0.4662     | 0.0311     | -15.0       | < 0.0001 |\n",
    "| **horsepower^2**   | 0.0012      | 0.0001     | 10.1        | < 0.0001 |\n",
    "\n",
    "\n",
    "But if including $\\text{horsepower}^2$ led to such a big improvement in the model, why not include $\\text{horsepower}^3$, $\\text{horsepower}^4$, or even $\\text{horsepower}^5$? It turns out the resulting fit from including all polynomials up to $\\text{horsepower}^5$ is too flexible, as shown in the red line in the figure. Here, it is obvious that the quadratic polynomial ($\\text{horsepower}^2$) is the best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Problems\n",
    "Most common problems when fiting a linear regression model to a data set are:\n",
    "\n",
    "1. Non-linearity of the response-predictor relationships.\n",
    "2. Correlation of error terms.\n",
    "3. Non-constant variance of error terms.\n",
    "4. Outliers.\n",
    "5. High-leverage points.\n",
    "6. Collinearity.\n",
    "\n",
    "\n",
    "### 1. Non-linearity of the Data\n",
    "Residual plots are a useful graphical tool for identifying non-linearity. Given a simple linear regression model, we can plot the residuals, $e_i = y_i − \\hat{y}_i$, versus the predictor $x_i$. In the case of a multiple regression model, since there are multiple predictors, we instead plot the residuals versus the predicted (or fitted) values $\\hat{y}_i$. **Ideally, the residual plot will show no discernible pattern.**\n",
    "\n",
    "If the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as $\\log{X}, \\sqrt{X}$, and $X^2$, in the regression model.\n",
    "\n",
    "### 2. Correlation of Error Terms\n",
    "An important assumption of the linear regression model is that the error terms, $\\epsilon_1 , \\epsilon_2, . . . , \\epsilon_n$, are uncorrelated. If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors and we may have an unwarranted sense of confidence in our model.\n",
    "\n",
    "Why might correlations among the error terms occur? Such correlations frequently occur in the context of *time series* data, which consists of observations for which measurements are obtained at discrete points in time. In many cases, observations that are obtained at adjacent time points will have positively correlated errors. If the error terms are positively correlated, then we may see **tracking** in the residuals—that is, **adjacent residuals may have similar values.**\n",
    "\n",
    "Many methods have been developed to properly take account of correlations in the error terms in time series data. Correlation among the error terms can also occur outside of time series data. In general, the assumption of uncorrelated errors is extremely important for linear regression as well as for other statistical methods, and good experimental design is crucial in order to mitigate the risk of such correlations.\n",
    "\n",
    "### 3. Non-constant Variance of Error Terms\n",
    "Another important assumption of the linear regression model is that the error terms have a constant variance, $\\text{Var}(\\epsilon_i ) = \\sigma^2$. The standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption.\n",
    "\n",
    "One can identify non-constant variances in the errors, or **heteroscedasticity**, from the presence of a funnel shape in the residual plot. When faced with this problem, one possible solution is to transform the response $Y$ using a concave function such as $\\log{Y}$ or $\\sqrt{Y}$.\n",
    "\n",
    "Sometimes we have a good idea of the variance of each response. For example, the $i$th response could be an average of $n_i$ raw observations. If each of these raw observations is uncorrelated with variance $\\sigma^2$ , then their average has variance $\\sigma_i^2 = \\sigma^2 / n_i$. In this case a simple remedy is to fit our model by weighted least squares, with weights proportional to the inverse variances—i.e. $w_i = n_i$ in this case.\n",
    "\n",
    "### 4. Outliers\n",
    "An *outlier* is a point for which $y_i$ is far from the value predicted by the outlier model. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit. However, even if an outlier does not have much effect on the least squares fit, it can cause other problems.\n",
    "\n",
    "One such problem is in the way it affects the $\\text{RSE}$ and the $R^2$ statistic. Since the RSE is used to compute all confidence intervals and  p-values, such a dramatic increase caused by a single data point can have implications for the interpretation of the fit.\n",
    "\n",
    "Residual plots can be used to identify outliers but in practice, it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the **studentized residuals**, computed by dividing each residual $e_i$ by its estimated standard error. Observations whose studentized residuals are greater than 3 in absolute value are possible outliers.\n",
    "\n",
    "If we believe that an outlier has occurred due to an error in data collection or recording, then one solution is to simply remove the observation. However, care should be taken, since an outlier may instead indicate a deficiency with the model, such as a missing predictor.\n",
    "\n",
    "### 5. High-leverage points\n",
    "We just saw that outliers are observations for which the response $y_i$ is unusual given the predictor $x_i$. In contrast, observations with *high leverage* have an unusual value for $x_i$. High leverage observations tend to have a sizable impact on the estimated regression line.\n",
    "\n",
    "In a simple linear regression, high leverage observations are fairly easy to\n",
    "identify, since we can simply look for observations for which the predictor\n",
    "value is outside of the normal range of the observations. But in a multiple\n",
    "linear regression with many predictors, it is possible to have an observation\n",
    "that is well within the range of each individual predictor’s values, but that\n",
    "is unusual in terms of the full set of predictors.\n",
    "\n",
    "In order to quantify an observation’s leverage, we compute the **leverage statistic**. A large value of this statistic indicates an observation with high leverage. For a simple linear regression,\n",
    "\n",
    "\\begin{equation}\\label{3.37}\n",
    "    h_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{ \\sum_{i^{\\prime} = 1}^n(x_{i^{\\prime}} - \\bar{x})^2 }.\n",
    "    \\tag{3.37}\n",
    "\\end{equation}\n",
    "\n",
    "The leverage statistic $h_i$ is always between $1/n$ and $1$, and *the average leverage for all the observations is always equal to* $(p + 1)/n$. So if a given observation has a leverage statistic that greatly exceeds $(p+ 1)/n$, then we may suspect that the corresponding point has high leverage.\n",
    "\n",
    "\n",
    "### 6. Collinearity\n",
    "*Collinearity* refers to the situation in which two or more predictor variables are closely related to one another. The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. In other words, since a credit card limit and credit rating tend to increase or decrease together, it can be difficult to determine how each one separately is associated with the response, balance.\n",
    "\n",
    "Graphing contour plots for the RSS values as a function of the parameters $\\beta$ for various regressions can sometimes identify whether two predictors are highly collinear. We might expect a number of concentric ellipsis about the optimal RSS value for two parameters that are not collinear. If two parameters are highly collinear, then we might still see ellipsises, except these will have very high eccentricity, resulting in more pairs of parameters with a similar value for RSS.\n",
    "\n",
    "Since collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for $\\hat{\\beta}_j$ to grow. Recall that the t-statistic for each predictor is calculated by dividing $\\hat{\\beta}_j$ by its standard error. Consequently, collinearity results in a decline in the t-statistic. As a\n",
    "result, in the presence of collinearity, we may fail to reject $H_0: \\beta_j = 0$. This means that the *power* of the hypothesis test—the probability of correctly detecting a *non-zero* coefficient—is reduced by collinearity.\n",
    "\n",
    "A simple way to detect collinearity is to look at the correlation matrix of the predictors. Unfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation **multicollinearity**.\n",
    "\n",
    "Instead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the **variance inflation factor** (VIF). The VIF is the ratio of the variance of $\\hat{\\beta}_j$ when fitting the full model divided by the variance of $\\hat{\\beta}_j$ if fit on its own. The smallest possible value for VIF is 1, which indicates the complete absence of collinearity. As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. The VIF for each variable can be computed using the formula\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{VIF}(\\hat{\\beta}_j) = \\frac{1}{ 1 - R^2_{ X_j | X_{-j} } },\n",
    "\\end{align*}\n",
    "\n",
    "where $R^2_{ X_j | X_{-j} }$ is the $R^2$ from a regression of $X_j$ onto all of the other predictors. If $R^2_{ X_j | X_{-j} }$ is close to one, then collinearity is present, and so the VIF will be large.\n",
    "\n",
    "When faced with the problem of collinearity, there are two simple solutions:\n",
    "\n",
    "The first is to drop one of the problematic variables from the regression. This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables. The second solution is to combine the collinear variables together into a single predictor. For instance, we might take the average of standardized versions of credit limit and credit rating in order to create a new variable that measures credit worthiness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Linear Regression with K-Nearest Neighbors\n",
    "Linear regression is an example of a parametric approach because it assumes a linear functional form for $f(X)$. Parametric methods have several advantages. They are often easy to fit, because one need estimate only a small number of coefficients with sometimes simple interpretations, and tests of statistical significance can be easily performed. But parametric methods do have a disadvantage: by construction, they make strong assumptions about the\n",
    "form of $f(X)$.\n",
    "\n",
    "In contrast, non-parametric methods do not explicitly assume a parametric form for $f(X)$, and thereby provide an alternative and more flexible approach for performing regression. The **K-nearest neighbors regression** (KNN) regression method is closely related to the KNN classifier.\n",
    "\n",
    "Given a value for $K$ and a prediction point $x_0$, KNN regression first identifies the $K$ training observations that are closest to $x_0$, represented by $N_0$ . It then estimates $f(x_0)$ using the average of all the training responses in $N_0$.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{f}(x_0) = \\frac{1}{K} \\sum_{x_i \\in N_0} y_i\n",
    "\\end{align*}\n",
    "\n",
    "In general, the optimal value for $K$ will depend on the *bias-variance tradeoff*, which we introduced in Chapter 2. A small value for K provides the most flexible fit, which will have low bias but high variance. In contrast, larger values of $K$ provide a smoother and less variable fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. In Chapter 5, we introduce several approaches for estimating test error rates. These methods can be used to identify the optimal value of $K$ in KNN regression.\n",
    "\n",
    "In what setting will a parametric approach such as least squares linear regression outperform a non-parametric approach such as KNN regression? The answer is simple: **the parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of $f$**.\n",
    "\n",
    "It seems obvious that linear regression is going to be better than a non-parametric approach on data with a truly linear relationship: a non-parametric approach incurs a cost in variance that is not offset by a reduction in bias. But what about a non-linear relationship?\n",
    "\n",
    "In a slight non-linear relationship we might still see that the test MSE for linear regression is still superior\n",
    "to that of KNN for low values of $K$, while KNN regression doesn't become competitive until larger values of $K$.\n",
    "\n",
    "In a highly non-linear relationship, we can start to expect that KNN substantially outperforms linear regression for all values of $K$. As the extent of non-linearity increases, there is little change in the test set MSE for the non-parametric KNN method, but there is a large increase in the test set MSE of linear regression.\n",
    "\n",
    "In a real life situation in which the true relationship is unknown, one might draw the conclusion that KNN should be favored over linear regression because it will at worst be slightly inferior than linear regression if the true relationship is linear, and may give substantially better results if the true relationship is non-linear. *But in reality, even when the true relationship is highly non-linear, KNN may still provide inferior results to linear regression.* This is especially so in the case of a response with many predictors.\n",
    "\n",
    "This decrease in performance as the dimension increases is a common problem for KNN, and results from the fact that in higher dimensions there is effectively a reduction in sample size. 100 training observations; when $p = 1$ may provide enough information to accurately estimate $f(X)$. However, spreading 100 observations over $p = 20$ dimensions results in a phenomenon in which a given observation has no \"nearby neighbors\"—this is the so-called **curse of dimensionality**. That is, the $K$ observations that are nearest to a given test observation $x_0$ may be very far away from $x_0$ in p-dimensional space when $p$ is large, leading to a very poor prediction of $f(x_0)$ and hence a poor KNN fit. As a general rule, **parametric methods will tend to outperform non-parametric approaches\n",
    "when there is a small number of observations per predictor**.\n",
    "\n",
    "Even in problems in which the dimension is small, we might prefer linear regression to KNN from an interpretability standpoint. If the test MSE of KNN is only slightly lower than that of linear regression, we might be willing to forego a little bit of prediction accuracy for the sake of a simple model that can be described in terms of just a few coefficients, and for which p-values are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# End Chapter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
