{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "1. [What Is Statistical Learning](#What-Is-Statistical-Learning)\n",
    "2. [Assessing Model Accuracy](#Assessing-Model-Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Statistical Learning\n",
    "Suppose that we observe a quantitative response $Y$ and $p$ different predictors, $X_1, X_2 , . . . , X_p$.\n",
    "\n",
    "We assume that there is some relationship between $Y$ and $X = (X_1 , X_2 , . . . , X_p )$, which can be written in the very general form\n",
    "\n",
    "\\begin{equation}\\label{2.1}\n",
    "    Y = f(X) + \\epsilon\n",
    "    \\tag{2.1}\n",
    "\\end{equation}\n",
    "\n",
    "where $f$ is some fixed but unknown function of $X_1, X_2 , . . . , X_p$ and $\\epsilon$ is a random error term with a mean of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Vs Interference\n",
    "In many situations, a set of inputs $X$ are readily available, but the output $Y$ cannot be easily obtained. Since the error term averages to zero, we can predict $Y$ using\n",
    "\n",
    "\\begin{equation}\\label{2.2}\n",
    "    \\hat{Y} = \\hat{f}(X)\n",
    "    \\tag{2.2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{f}$ represents our estimate for $f$ , and $\\hat{Y}$ represents the resulting prediction for $Y$.\n",
    "\n",
    "### Prediction\n",
    "Prediction is more concerned about the accuracy of $Y$ and less about the *form* of $\\hat{f}$.\n",
    "\n",
    "The accuracy of $\\hat{Y}$ as a prediction for $Y$ depens on two quantities, **reducible error** and **irreducible error**.\n",
    "\n",
    "- Reducible error can be decreased by improving the accuracy of $\\hat{f}$ by using the most appropriate statisticcal learning technique.\n",
    "- Irreducible error comes from $\\epsilon$ itself and cannot be reduced by improving $\\hat{f}$\n",
    "\n",
    "The **Expected Value** of the quared difference between the predicted and actual value of $Y$ is shown below\n",
    "\n",
    "\\begin{equation}\\label{2.3}\n",
    "    E(Y - \\hat{Y})^2= E [ f(X) + \\epsilon - \\hat{f}(X) ]^2 \\\\\n",
    "    = \\underbrace{ [ f(X) - \\hat{f}(X) ]^2 }_{Reducible} + \\underbrace{\\text{Var}(\\epsilon)}_{Irreducible}, \n",
    "    \\tag{2.3}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\text{Var}(\\epsilon)$ represents the variance associated with the error term $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "If we are more interested in understanding that $Y$ is affected as $X_1, X_2 , . . . , X_p$ change, we may wish to estimate $f$ truer to its exact form.\n",
    "\n",
    "One may be interested in answering the following questions:\n",
    "- Which predictors $( X_m )$ are associated with the response?\n",
    "- What is the relationship between the response and each predictor?\n",
    "- Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating $f$\n",
    "The methods for estimating $f$ can generally be divided into two categories.\n",
    "\n",
    "### Parametric Method\n",
    "This method involves a two-step model-based approach.\n",
    "1. we make an assumption about the functional form, or shape, of $f$. For example, one very simple assumption is that f is linear in $X$:\n",
    "\\begin{equation}\\label{2.4}\n",
    "    f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p.\n",
    "    \\tag{2.4}\n",
    "\\end{equation}\n",
    "Then estimate the coefficients.\n",
    "2. After a model has been selected, we need a procedure that uses the training data to *fit* or *train* the model.\n",
    "\n",
    "In the case of the above example, we might use the process of **least squares**.\n",
    "\n",
    "### Non-parametric Methods\n",
    "This method does not make explicit assumptions about the functional form of $f$. Instead, this method seeks to estimate $f$ as close to the data points as possible without overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Vs Unsupervised Learning\n",
    "For **Supervised Learning**, each observation of the predictor measurement(s) $x_i$, where $i = 1, ... , n$ there is an associated response measurement $y_i$. Many classical learning methods such as *linear regression* and *logistic regression*, *generalized additive model, boosting*, and *vector machines* operate in the supervised learning domain.\n",
    "\n",
    "In contrast, **Unsupervised Learning** has the situation that for every observation $i = 1, ... , n$ we observe a vector of measurements $x_i$ but no associated response. We seek to understand the relationships between the variables or between the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Vs Classification Problems\n",
    "Variables can be characterized as either **quantitative** or **qualitative** (also known as *categorical*).\n",
    "\n",
    "Problems with *quantitative* response tend to be **regression problems**, while those involving a *qualitative* response are often referred to as **classification problems**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Model Accuracy\n",
    "## Measuring the quality of fit\n",
    "We need some way to measure how well its predictions actually match the observed data.\n",
    "\n",
    "In the regression setting, the most commonly-used measure is the **mean squared error** (MSE), given by\n",
    "\n",
    "\\begin{equation}\\label{2.5}\n",
    "    MSE = \\frac{1}{n} \\sum^n_{i=1} (y_i - \\hat{f}(x_i))^2\n",
    "    \\tag{2.5}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{f}(x_i)$ is the prediction that $\\hat{f}$ gives for the $i$th observation.\n",
    "\n",
    "It is important to note that *this MSE is computed using the training data*, i.e. **Training MSE**. Generally, we are more interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data. For example, predicting future stock prices instead of last week's.\n",
    "\n",
    "So it is important to distinguish that we want to choose a method that gives the lowest *test MSE*, as opposed to the lowest training MSE as in (\\ref{2.5}).\n",
    "\n",
    "If we had a large number of test observations, we could compute **Test MSE** as\n",
    "\n",
    "\\begin{equation}\\label{2.6}\n",
    "    \\text{Ave}( y_0 - \\hat{f}(x_0) )^2\n",
    "    \\tag{2.6}\n",
    "\\end{equation}\n",
    "\n",
    "the average squared prediction error for these test observations, $(x_0 , y_0)$, previously unseen.\n",
    "\n",
    "Overfitting data occurs when a given method yields a small training MSE but a large test MSE. Generally, a less flexible method (fewer degrees of freedom) would have yielded a smaller test MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias-Variance Trade-Off\n",
    "It is possible to show that the expected test MSE, for a given value $x_0$, can always be decomposed into the sum of three fundamental quantities:\n",
    "\n",
    "1. the **variance** of $\\hat{f}(x_0)$,\n",
    "2. the squared **bias** of $\\hat{f}(x_0)$ and\n",
    "3. the variance of the error terms $\\epsilon$.\n",
    "\n",
    "That is,\n",
    "\\begin{equation}\\label{2.7}\n",
    "    E \\left( y_0 - \\hat{f}(x_0) \\right)^2 = \\text{Var}( \\hat{f}(x_0)) + [\\text{Bias}(\\hat{f}(x_0))]^2 + \\text{Var}(\\epsilon).\n",
    "    \\tag{2.7}\n",
    "\\end{equation}\n",
    "\n",
    "Here we define the **expected test MSE**, and refers to the average test MSE that we would obtain if we repeatedly estimated $f$ using a large number of training sets, and tested each at $x_0$.\n",
    "\n",
    "**Variance Term**: This term refers to the amount by which $\\hat{f}$ would change if we estimated it using a different training data set. Generally, fewer degrees of freedom results in smaller variance when different training data is used for $\\hat{f}$.\n",
    "\n",
    "**Bias Term**: This term refers to the error that is introduced by approximating a real life problem, by a much simpler model. For example, linear regression as a model for almost anything. Generally, more degrees of freedom results in less bias.\n",
    "\n",
    "As we increase flexibility (degrees of freedom) of a class of methods, the bias tends to initially decrease faster than the variance increases. However, at some point increasing flexibility has little impact on bias but starts to significantly increase the variance. The test MSE can be optimized by finding the turning point.\n",
    "\n",
    "This is called **Bias-Variance trade off**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Classification Setting\n",
    "Suppose that we seek to estimate f on the basis of training observations $\\{(x_1 , y_1 ), . . . , (x_n , y_n)\\}$, where now $y_1, . . . , y_n$ are qualitative rather than numerical. The most common approach for quantifying the accuracy of our estimate $\\hat{f}$ is the **training error rate**, the proportion of mistakes that are made if we apply our estimate $\\hat{f}$ to the training observations:\n",
    "\n",
    "\\begin{equation}\\label{2.8}\n",
    "    \\frac{1}{n} \\sum^n_{i = 1} I(y_i \\neq \\hat{y}_i)\n",
    "    \\tag{2.8}\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\hat{y}_i$ is the predicted class label for the $i$th observation using $\\hat{f}$. And $I(y_i \\neq \\hat{y}_i )$ is an indicator variable that equals $1$ if $y_i \\neq \\hat{y}_i$ and zero if $y_i = \\hat{y}_i$. That is, $I(y_i \\neq \\hat{y}_i) = 0$ when the $i$th observation is correctly classified.\n",
    "\n",
    "The **test error rate** associated with a set of test observations of the form test error $(x_0 , y_0 )$ is given by\n",
    "\n",
    "\\begin{equation}\\label{2.9}\n",
    "    \\text{Ave}(I(y_0 \\neq \\hat{y}_0)),\n",
    "    \\tag{2.9}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{y}_0$ is the predicted class label that results from applying the classifier to the test observation with predictor $x_0$. A *good* classifier is one for which the test error (\\ref{2.9}) is smallest.\n",
    "\n",
    "We will cover two classifying methods in this chapter\n",
    "\n",
    "- Bayes Classifier\n",
    "- K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bayes Classifier\n",
    "It is possible to show that the test error rate given in (\\ref{2.9}) is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values. In other words, we should simply assign a test observation with predictor vector $x_0$ to the class $j$ for which\n",
    "\n",
    "\\begin{equation}\\label{2.10}\n",
    "    \\text{Pr}(Y = j | X = x_0)\n",
    "    \\tag{2.10}\n",
    "\\end{equation}\n",
    "\n",
    "is largest. Note that (\\ref{2.10}) is a conditional probability: it is the probability conditional that $Y = j$, given the observed predictor vector $x_0$. This very simple classifier is called the **Bayes classifier**.\n",
    "\n",
    "The Bayes Classifier produces the lowest possible test error rate, called the **Bayes error rate**. Since the Bayes classifier will always choose the class for which (\\ref{2.10}) is largest, the error rate at $X = x_0$ will be $1−\\text{max}_j Pr(Y = j|X = x_0 )$. In general, the overall Bayes error rate is given by\n",
    "\n",
    "\\begin{equation}\\label{2.11}\n",
    "    1 - E \\left( \\text{max}_j \\text{Pr}(Y = j | X)  \\right),\n",
    "    \\tag{2.11}\n",
    "\\end{equation}\n",
    "\n",
    "where the expectation averages the probability over all possible values of $X$. Because classes overlap in the true population so $\\text{max}_j \\text{Pr}(Y = j | X) < 1$ for some values of $x_0$. The Bayes error rate is analogous to the irreducible error, discussed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "Often we do not know the conditional distribution of $Y$ given $X$, and so computing the Bayes classifier is impossible. One method of estimating the conditional distribution of the $Y$ given $X$ is the **K-Nearest Neighbors** (KNN) classifier.\n",
    "\n",
    "Given a positive integer $K$ and a test observation $x_0$, the KNN classifier first identifies the $K$ points in the training data that are closest to $x_0$, represented by $N_0$. It then estimates the conditional probability for class $j$ as the fraction of points in $N_0$ whose response values equal $j$:\n",
    "\n",
    "\\begin{equation}\\label{2.12}\n",
    "    \\text{Pr}(Y = j | X = x_0) = \\frac{1}{K} \\sum_{i \\in N_0} I(y_i = j).\n",
    "    \\tag{2.12}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# End Chapter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
