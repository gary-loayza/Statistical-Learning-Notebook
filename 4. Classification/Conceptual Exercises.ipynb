{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Using a little bit of algebra, prove that**\n",
    "\\begin{align*}\n",
    "    p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{ 1 + e^{\\beta_0 + \\beta_1 X}}\n",
    "\\end{align*}\n",
    "**is equivalent to** \n",
    "\\begin{align*}\n",
    "    \\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "**In other words, prove the logistic function representation and logit representation for the logistic regression model are equivalent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    p(X) &= \\frac{e^{\\beta_0 + \\beta_1 X}}{ 1 + e^{\\beta_0 + \\beta_1 X}} \\newline\n",
    "    \\frac{p(X)}{1 - p(X)} &= \\left[ \\frac{e^{\\beta_0 + \\beta_1 X}}{ 1 + e^{\\beta_0 + \\beta_1 X}} \\right] \\left[ 1 - \\frac{e^{\\beta_0 + \\beta_1 X}}{ 1 + e^{\\beta_0 + \\beta_1 X}} \\right]^{-1} \\newline\n",
    "     &= \\left[ \\frac{e^{\\beta_0 + \\beta_1 X}}{ 1 + e^{\\beta_0 + \\beta_1 X}} \\right] \\left[ \\frac{1 + e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} - \\frac{e^{\\beta_0 + \\beta_1 X}}{ 1 + e^{\\beta_0 + \\beta_1 X}} \\right]^{-1} \\newline\n",
    "     &= \\left[ \\frac{e^{\\beta_0 + \\beta_1 X}}{ 1 + e^{\\beta_0 + \\beta_1 X}} \\right] \\left[ \\frac{1}{1 + e^{\\beta_0 + \\beta_1 X}} \\right]^{-1} \\newline\n",
    "     &= \\left[ \\frac{e^{\\beta_0 + \\beta_1 X}}{ 1 + e^{\\beta_0 + \\beta_1 X}} \\right] \\left[ 1 + e^{\\beta_0 + \\beta_1 X} \\right] \\newline\n",
    "     \\frac{p(X)}{1 - p(X)} &= e^{\\beta_0 + \\beta_1 X}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. It was stated in the text that classifying an observation to the class for which**\n",
    "\\begin{align*}\n",
    "    p_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\text{exp}(- \\frac{1}{2 \\sigma^2} (x - \\mu_k)^2 )}{\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\text{exp}(- \\frac{1}{2 \\sigma^2} (x - \\mu_l)^2 )} \n",
    "\\end{align*}\n",
    "**is largest is equivalent to classifying an observation to the class for which**\n",
    "\n",
    "\\begin{align*}\n",
    "    \\delta_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2} + \\text{log}(\\pi_k)\n",
    "\\end{align*}\n",
    "\n",
    "**is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a** $N(\\mu_k , \\sigma^2 )$ **distribution, the Bayes’ classifier assigns an observation to the class for which the discriminant function is maximized.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by trying to maximize $p_k(x)$. First, we will need to recognize that the denominator will be the same for all $k$, so we will ignore it in our maximization. We are left with\n",
    "\n",
    "\\begin{align*}\n",
    "    p_k^*(x) = \\pi_k \\frac{1}{\\sqrt{2 \\pi }\\sigma} \\exp \\left(-\\frac{1}{2 \\sigma^2}(x - \\mu_k)^2 \\right)\\, .\n",
    "\\end{align*}\n",
    "\n",
    "Additionally, the $\\frac{1}{\\sqrt{2 \\pi} \\sigma}$ term is constant, so we just need to maximize\n",
    "\n",
    "\\begin{align*}\n",
    "    p_k^{**}(x) = \\pi_k \\exp \\left(-\\frac{1}{2 \\sigma^2}(x - \\mu_k)^2 \\right)\\, .\n",
    "\\end{align*}\n",
    "\n",
    "Now we may want to take the logarithm to make it easier to take the derivative. But we must recognize that logarithms preserve order. In other words, if $p^{**}_i(x) < p^{**}_j(x)$, then $\\log(p^{**}_i(x)) < \\log(p^{**}_j(x))$. This is important because it means our problem remains a maximization problem, and does not become a minimization problem. Taking a log of $p^{**}_k(x)$ gives\n",
    "\n",
    "\\begin{align*}\n",
    "    \\log(p^{**}_k(x)) &= \\log \\left( \\pi_k \\exp \\left(-\\frac{1}{2 \\sigma^2}(x - \\mu_k)^2 \\right) \\right) \\\\\n",
    "    &= \\log(\\pi_k) -\\frac{1}{2 \\sigma^2}(x - \\mu_k)^2 \\\\\n",
    "    &= \\log(\\pi_k) -\\frac{1}{2 \\sigma^2}(x^2 - 2x \\mu_k + \\mu_k^2) \\\\\n",
    "    &= \\log(\\pi_k) - \\frac{x^2}{2 \\sigma^2} + \\frac{x \\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2} \\, .\n",
    "\\end{align*}\n",
    "\n",
    "Since $\\sigma^2$ is constant, we have that the $\\frac{x^2}{2\\sigma^2}$ term is constant in $k$, so we can ignore it when maximizing $p_k(x)$. Removing this term from the last equation gives\n",
    "\n",
    "\\begin{align*}\n",
    "    \\delta_k(x) = \\log(\\pi_k) + \\frac{x \\mu_k}{\\sigma^2} -\\frac{\\mu_k^2}{2 \\sigma^2} \\, .\n",
    "\\end{align*}\n",
    "\n",
    "From this we can conclude that maximizing our original equation is equivalent to maximizing the last equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class-specific mean vector and a class specific covariance matrix. We consider the simple case where $p = 1$; i.e. there is only one feature. Suppose that we have K classes, and that if an observation belongs to the kth class then X comes from a one-dimensional normal distribution, $X \\sim N(\\mu_k , \\sigma_k^2 )$. Recall that the density function for the one-dimensional normal distribution is given in (4.11). Prove that in this case, the Bayes’ classifier is not linear. Argue that it is in fact quadratic.**\n",
    "\n",
    "*Hint: For this problem, you should follow the arguments laid out in **Linear Discriminant Analysis for p = 1**, but without making the assumption that* $\\sigma_1^2 = \\ldots = \\sigma_K^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the argument as in **Exercise 2**, but we will end up with\n",
    "\n",
    "\\begin{equation}\\label{4.1}\n",
    "    p_k^*(x) = \\pi_k \\frac{1}{\\sqrt{2 \\pi }\\sigma_k} \\exp \\left(-\\frac{1}{2 \\sigma^2}(x - \\mu_k)^2 \\right)\\, .\n",
    "\\tag{4.1}\n",
    "\\end{equation}\n",
    "\n",
    "Note that we can’t drop the $\\sigma_k$ as in **Exercise 2.** since it is unique for each population.  \n",
    "Taking the log of the (\\ref{4.1}) we get\n",
    "\n",
    "\\begin{align}\n",
    "    \\log(p_k^*(x)) &= \\log(\\pi_k) - \\log(\\sqrt(2 \\pi) \\sigma_k) + \\frac{1}{2 \\sigma_k^2} (x - \\mu_k)^2 \\nonumber \\\\\n",
    "    &= \\log(\\pi_k) - \\log(\\sqrt(2 \\pi) \\sigma_k) + \\frac{1}{2 \\sigma_k^2} (x^2 - 2x\\mu_k + \\mu_k^2) \\nonumber \\\\\n",
    "\\end{align}\n",
    "\n",
    "Note that we cannot drop the $1/(2\\sigma^2_k)$ term since it varies between each group. Therefore, the above equation is quadratic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that\n",
    "non-parametric approaches often perform poorly when p is large. We will now investigate this curse.**\n",
    "\n",
    "**(a) Suppose that we have a set of observations, each with measurements on $p = 1$ feature, $X$. We assume that $X$ is uniformly (evenly) distributed on $[0, 1]$. Associated with each observation is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within $10\\%$ of the range of $X$ closest to that test observation. For instance, in order to predict the response for a test observation with $X = 0.6$, we will use observations in the range $[0.55, 0.65]$. On average, what fraction of the available observations will we use to make the prediction?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is essentially asking us to compute $P(|X_{\\text{test}}-X_{\\text{train}}|<.05)$ which we can estimate using the example provided. Given some $X^{\\prime} = x_{\\text{test}}$, the probability that our observation falls within $\\pm 0.05$ of $x_{\\text{train}}$ is $P(x_{\\text{train}}-0.05 \\leq X^{\\prime} \\leq x_{\\text{train}}+0.05) = \\frac{0.65 - 0.55}{1-0} = 0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Now suppose that we have a set of observations, each with measurements on p = 2 features, X1 and X2 . We assume that $(X_1 , X_2 )$ are uniformly distributed on $[0, 1] \\times [0, 1]$. We wish to predict a test observation’s response using only observations that are within $10\\%$ of the range of $X_1$ and within $10\\%$ of the range of $X_2$ closest to that test observation. For instance, in order to predict the response for a test observation with $X_1 = 0.6$ and $X_2 = 0.35$, we will use observations in the range $[0.55, 0.65]$ for $X_1$ and in the range $[0.3, 0.4]$ for $X_2$. On average, what fraction of the available observations will we use to make the prediction?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, given test observation $X^{\\prime} = (\\hat{x}_1,\\hat{x}_2)$ and $X_{\\text{ train}} = (x_1, x_2) \\in \\mathbb{R}^2$,  \n",
    "$P(x_1 - 0.05 \\leq \\hat{x}_1 \\leq x_1 + 0.05 \\wedge\tx_2 - 0.05 \\leq \\hat{x}_2 \\leq x_2 + 0.05)$  \n",
    "$=P(x_1 - 0.05 \\leq \\hat{x}_1 \\leq x_1 + 0.05)P(x_2 - 0.05 \\leq \\hat{x}_2 \\leq x_2 + 0.05)$  \n",
    "$=P(x_1 - 0.05 \\leq \\hat{x}_1 \\leq x_1 + 0.05)^2 = 0.1^2 = 0.01$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Now suppose that we have a set of observations on $p = 100$ features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from $0$ to $1$. We wish to predict a test observation’s response using observations within the $10\\%$ of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part (b) we saw that it is just the probability for the single dimension case, raised to the power of $p$.\n",
    "So in this case, we can expect the fraction to be $(0.1)^{100} \\approx 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Using your answers to parts (a)–(c), argue that a drawback of KNN when p is large is that there are very few training observations “near” any given test observation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have many training observations it is very likely you can find a value, $X = x_i$, in your training data that is close to the value for which you are trying to predict, test observation $X^{\\prime}$.\n",
    "\n",
    "But with many dimensions for each observation, it will be less likely to find a value where $X = (x_i, x_j, \\ldots, x_k)$ is close to test observation $X^{\\prime}$.\n",
    "\n",
    "Pair this with the need for many similarly close values required to make a good prediction and we can see how KNN is affected by large p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Now suppose that we wish to make a prediction for a test observation by creating a p-dimensional hypercube centered around the test observation that contains, on average, $10\\%$ of the training observations. For $p = 1, 2,$ and $100$, what is the length of each side of the hypercube? Comment on your answer.**\n",
    "\n",
    "*Note: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When $p = 1$, a hypercube is simply a line segment, when $p = 2$ it is a square, and when $p = 100$ it is a 100-dimensional cube.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $l$ is the length of the hypercube $l \\leq 1$, then\n",
    "$P(x_1 - \\frac{l}{2} \\leq \\hat{x}_1 \\leq x_1 + \\frac{l}{2}, \\dots, x_p - \\frac{l}{2} \\leq \\hat{x}_p \\leq x_p + \\frac{l}{2}) = l^p$.  \n",
    "Thus, if we want 10%, we need to find $\\sqrt[p]{.1} = l$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 1 \t: l = 0.1\n",
      "p = 2 \t: l = 0.31622776601683794\n",
      "p = 100 : l = 0.9772372209558107\n"
     ]
    }
   ],
   "source": [
    "def side_length(p):\n",
    "    return ( (0.1)**(1/p) )\n",
    "\n",
    "print(\"p = 1 \\t: l =\", side_length(1))\n",
    "print(\"p = 2 \\t: l =\", side_length(2))\n",
    "print(\"p = 100 : l =\", side_length(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. We now examine the differences between LDA and QDA.**\n",
    "\n",
    "**(a) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect LDA to perform better on the training set and the test set for a linear Bayes decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we would expect QDA to perform better on the training set up to the point that the level that the boundary is non-linear is surpassed by the level of flexibility of QDA model. The same would be true for the test set. Unless the Bayes boundary was only slightly non-linear, then QDA is going to be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) In general, as the sample size n increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small sample sizes, QDA will perform worse on test data than LDA, but we also expect QDA to improve as n increases. See an explanation I found below.\n",
    "\n",
    "> \"We would expect the test accuracy of QDA to improve versus LDA. This is because as we get more $n$, we get much better estimates to $\\sigma_k$. If it turns out that $\\sigma_k = \\sigma$ for all $k$, then QDA will still capture this really well. Especially since variance estimates have diminishing returns relative to $n$.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False. If the Bayes decision boundary is linear, then we know that for all $k$, $\\sigma_k = \\sigma$. So when we estimate $\\sigma$, we can use the observations from all the different populations. But when we do QDA, we have to estimate $\\sigma_k$ differently for each $k$. This means that we have to divide up our observations to estimate each $\\sigma_k$. Thus, when using QDA, we might have a bunch of $\\sigma_k$ that are slightly off from $\\sigma$, creating a source of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Suppose we collect data for a group of students in a statistics class with variables** $X_1 = \\text{ hours studied}, X_2 = \\text{ undergrad GPA}$**, and** $Y = \\text{ receive an A}$**. We fit a logistic regression and produce estimated coefficient,** $\\hat{\\beta}_0 = −6, \\hat{\\beta}_1 = 0.05, \\hat{\\beta}_2 = 1$.\n",
    "\n",
    "**(a) Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Logistic function, we have\n",
    "\n",
    "\\begin{align*}\n",
    "    p(X) = \\frac{e^{-6 + 0.05 X_1 + 1 X_2}}{ 1 + e^{-6 + 0.05 X_1 + 1 X_2}}.\n",
    "\\end{align*}\n",
    "\n",
    "Plugging in for $X_1 = 40 \\text{hours}$ and $X_2 = 3.5$, we get\n",
    "\n",
    "\\begin{align*}\n",
    "    p(40 \\text{h}, 3.5) \\approx \\frac{0.607}{1 + 0.607} = 0.378\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) How many hours would the student in part (a) need to study to have a $50 \\%$ chance of getting an A in the class?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From part (a) we have\n",
    "\n",
    "\\begin{align*}\n",
    "    p(X_1, 3.5) &= \\frac{e^{-6 + 0.05 X_1 + 3.5}}{ 1 + e^{-6 + 0.05 X_1 + 3.5}} = 0.50\n",
    "\\end{align*}\n",
    "\n",
    "from (4.6) we have\n",
    "\n",
    "\\begin{align*}\n",
    "    \\log{ \\left( \\frac{p(X)}{1 - p(X)} \\right) } = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p\n",
    "\\end{align*}\n",
    "\n",
    "which we can use to isolate $X_1$\n",
    "\n",
    "\\begin{align*}\n",
    "    -6 + 0.05 X_1 + 3.5 &= \\log{ \\left( \\frac{0.50}{1 - 0.50} \\right) } \\newline\n",
    "    -2.5 + 0.05 X_1 &= 0 \\newline\n",
    "    0.05 X_1 &= 2.5 \\newline\n",
    "    X_1 &= \\frac{2.5}{0.05} = 50 \\text{ hours} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Suppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based  \n",
    "on $X$, last year’s percent profit. We examine a large number of companies and discover that the mean value of $X$ for companies that issued a dividend was $\\bar{X} = 10$, while the mean for those that didn’t was $\\bar{X} = 0$. In addition, the variance of $X$ for these two sets of companies was $\\hat{\\sigma}^2 = 36$. Finally, $80 \\%$ of companies issued dividends. Assuming that $X$ follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was $X = 4$ last year.**\n",
    "\n",
    "*Hint: Recall that the density function for a normal random variable is the Gaussian*  \n",
    "$f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\text{exp} \\left( - \\frac{1}{2 \\sigma^{2}}(x - \\mu)^2 \\right)$  \n",
    "*You will need to use Bayes’ theorem.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) for k :\t 0.0403284540865239\n",
      "f(x) for l :\t 0.053241334253725375\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def gaussian(sigma_squared=36, mu=10, x=4):\n",
    "    constant = 1/(math.sqrt(2 * math.pi * sigma_squared))\n",
    "    e = math.exp((-1/(2*sigma_squared))*((x - mu)**2))\n",
    "    return constant*e\n",
    "\n",
    "print(\"f(x) for k :\\t\", gaussian())\n",
    "print(\"f(x) for l :\\t\", gaussian(mu=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that $\\pi_k = 0.80$, $f_k(x) \\approx 0.040$, and $f_l(x) \\approx 0.053$\n",
    "\n",
    "\\begin{align*}\n",
    "    p_k(X = 4) \\approx \\frac{0.80 \\cdot 0.040}{0.20 \\cdot 0.053 + 0.80 \\cdot 0.040} = 0.75\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Suppose that we take a data set, divide it into equally-sized training\n",
    "and test sets, and then try out two different classification procedures.\n",
    "First we use logistic regression and get an error rate of 20% on the\n",
    "training data and 30% on the test data. Next we use 1-nearest neighbors\n",
    "(i.e. K = 1) and get an average error rate (averaged over both\n",
    "test and training data sets) of 18%. Based on these results, which\n",
    "method should we prefer to use for classification of new observations?\n",
    "Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can expect the 1-nearest neighbor model to have $0\\%$ training error. This means that the test error, given that the train error was $0\\%$ and that $18\\%$ was the average, must have been $36\\%$.\n",
    "\n",
    "Logistic regression is the better model of these two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. This problem has to do with odds.**\n",
    "\n",
    "**(a) On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\frac{p(X)}{1 - p(X)} &= \\text{odds} \\newline\n",
    "    \\frac{p(X)}{1 - p(X)} &= 0.37 \\newline\n",
    "    p(X) &= 0.37 - 0.37 p(X) \\newline\n",
    "    p(X) + 0.37 p(X) &= 0.37 \\newline\n",
    "    p(X) &= \\frac{0.37}{1.37} = 0.27\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Suppose that an individual has a 16% chance of defaulting on\n",
    "her credit card payment. What are the odds that she will default?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\frac{p(X)}{1 - p(X)} &= \\text{odds} \\newline\n",
    "    \\frac{0.16}{1 - 0.16} &= 0.19 \n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
