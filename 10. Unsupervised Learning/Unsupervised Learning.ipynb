{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "1. [The Challenge of Unsupervised Learning](#The-Challenge-of-Unsupervised-Learning)\n",
    "2. [Principal Components Analysis](#Principal-Components-Analysis)\n",
    "3. [Clustering Methods](#Clustering-Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This chapter will focus on **unsupervised learning**, a set of statistical tools intended for the setting in which we have only a set of features $X_1, X_2 , \\ldots, X_p$ measured on $n$ observations. **We are not interested in prediction, because we do not have an associated response variable $Y$.** Rather, the goal is to discover interesting things about the measurements on $X_1, X_2 , \\ldots, X_p$.\n",
    "\n",
    "Is there an informative way to visualize the data? Can we discover subgroups among the variables or among the observations? Unsupervised learning refers to a diverse set of techniques for answering questions such as these.\n",
    "\n",
    "In this chapter, we will focus on two particular types of unsupervised learning: **principal components analysis**, a tool used for data visualization or data pre-processing before supervised techniques are applied, and **clustering**, a broad class of methods for discovering unknown subgroups in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Challenge of Unsupervised Learning\n",
    "Unsupervised learning tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Unsupervised learning is often performed as part of an **exploratory data analysis**.\n",
    "\n",
    "Furthermore, it can be hard to assess the results obtained from unsupervised learning methods, since there is no universally accepted mechanism for performing cross-validation or validating results on an independent data set. The reason for this difference is simple. If we fit a predictive model using a supervised learning technique, then it is possible to check our work by seeing how well our model predicts the response $Y$ on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don’t know the true answer—the problem is unsupervised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis\n",
    "When faced with a large set of correlated variables, principal components allow us to summarize this set with a smaller number of representative variables that collectively explain most of the variability in the original set. **The principal component directions are presented in Section 6.3.1 as directions in feature space along which the original data are highly variable**. These directions also define lines and subspaces that are as close as possible to the data cloud. To perform principal components regression, we simply use principal components as predictors in a regression model in place of the original larger set of variables.\n",
    "\n",
    "**Principal component analysis** (PCA) refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data. PCA is an unsupervised approach, since it involves only a set of features $X_1, X_2, \\ldots, X_p$, and no associated response $Y$.\n",
    "\n",
    "Apart from producing derived variables for use in supervised learning problems, PCA also serves as a tool for data visualization (visualization of the observations or visualization of the variables).\n",
    "\n",
    "## What Are Principal Components?\n",
    "Suppose that we wish to visualize $n$ observations with measurements on a set of $p$ features, $X_1, X_2, \\ldots, X_p$, as part of an exploratory data analysis. We could do this by examining two-dimensional scatterplots of the data, each of which contains the $n$ observations’ measurements on two of the features. If $p$ is large, then it will certainly not be possible to look at all of them; moreover, **most likely none of them will be informative since they each contain just a small fraction of the total information present in the data set**. Clearly, a better method is required to visualize the n observations when $p$ is large. In particular, we would like to find a low-dimensional representation of the data that captures as much of the information as possible.\n",
    "\n",
    "PCA provides a tool to do just this. **It finds a low-dimensional representation of a data set that contains as much as possible of the variation. The idea is that each of the $n$ observations lives in $p$-dimensional space, but not all of these dimensions are equally interesting**. PCA seeks a small number of dimensions that are as interesting as possible, where the concept of *interesting* is **measured by the amount that the observations vary along each dimension**. Each of the dimensions found by PCA is a linear combination of the $p$ features. We now explain the manner in which these dimensions, or *principal components*, are found.\n",
    "\n",
    "The *first principal component* of a set of features $X_1, X_2, \\ldots, X_p$ is the normalized linear combination of the features\n",
    "\n",
    "\\begin{equation}\\label{10.1}\n",
    "    Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 + \\ldots + \\phi_{p1}X_p\n",
    "    \\tag{10.1}\n",
    "\\end{equation}\n",
    "\n",
    "that has the largest variance.\n",
    "\n",
    "By *normalized*, we mean that $\\sum^p_{j=1}\\phi^2_{j1}=1$. We refer to the elements $\\phi_{11}, \\ldots, \\phi_{p1}$ as the loadings of the first principal component; together, the loadings make up the principal component loading vector, $\\phi_1 =(\\phi_{11} \\phi_{21} \\ldots \\phi_{p1})^T$. We constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance.\n",
    "\n",
    "Given a $n \\times p$ data set $\\boldsymbol{X}$, how do we compute the first principal component? Since we are only interested in variance, we assume that each of the variables in $\\boldsymbol{X}$ has been centered to have mean zero (that is, the column means of $\\boldsymbol{X}$ are zero). We then look for the linear combination of the\n",
    "sample feature values of the form\n",
    "\n",
    "\\begin{equation}\\label{10.2}\n",
    "    z_{i1} = \\phi_{11}x_{i1}+ \\phi_{21}x_{i2} + \\ldots + \\phi_{p1}x_{ip}\n",
    "    \\tag{10.2}\n",
    "\\end{equation}\n",
    "\n",
    "that has largest sample variance, subject to the constraint that $\\sum^p_{j=1} \\phi_{j1}^2 = 1$. In other words, the first principal component loading vector solves the optimization problem\n",
    "\n",
    "\\begin{equation}\\label{10.3}\n",
    "    { \\text{maximize} \\atop \\phi_{11}, \\ldots, \\phi_{p1} } \\left\\{ \\frac{1}{n} \\sum^n_{i=1} \\left( \\sum^p_{j=1} \\phi_{j1}x_{ij} \\right)^2 \\right\\} \\text{ subject to } \\sum^p_{j=1} \\phi^2_{j1} = 1\n",
    "    \\tag{10.3}\n",
    "\\end{equation}\n",
    "\n",
    "From (\\ref{10.2}) we can write the objective in (\\ref{10.3}) as $\\frac{1}{n} \\sum^n_{i=1} z_{i1}^2$. Since $\\frac{1}{n} \\sum^n_{i=1} x_{ij} = 0$, the average of the $z_{11}, \\ldots, z_{n1}$ will be zero as well. **Hence\n",
    "the objective that we are maximizing in (\\ref{10.3}) is just the sample variance of the $n$ values of $z_{i1}$**. We refer to $z_{11}, \\ldots, z_{n1}$ as the **scores** of the first principal component. Problem (\\ref{10.3}) can be solved via an eigen decomposition, a standard technique in linear algebra, but details are outside of the scope of this book.\n",
    "\n",
    "There is a nice geometric interpretation for the first principal component. The loading vector $\\phi_1$ with elements $\\phi_{11}, \\phi_{21}, \\ldots, \\phi_{p1}$ defines a direction in feature space along which the data vary the most. **If we project the $n$ data points $x_1, \\ldots, x_n$ onto this direction, the projected values are the principal component scores $z_{11}, \\ldots, z_{n1}$ themselves**.\n",
    "\n",
    "After the first principal component $Z_1$ of the features has been determined, we can find the second principal component $Z_2$. The second principal component is the linear combination of $X_1, \\ldots, X_p$ that has maximal variance out of all linear combinations that are **uncorrelated** with $Z_1$. The second principal component scores $z_{12}, z_{22}, \\ldots, z_{n2}$ take the form\n",
    "\n",
    "\\begin{equation}\\label{10.4}\n",
    "    z_{i2} = \\phi_{12}x_{i2}+ \\phi_{22}x_{i2} + \\ldots + \\phi_{p2}x_{ip}\n",
    "    \\tag{10.4}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\phi_2$  is the second principal component loading vector, with elements $\\phi_{12}, \\phi_{22}, \\ldots,\\phi_{p2}$. It turns out that constraining $Z_2$ to be uncorrelated with $Z_1$ is equivalent to constraining the direction $\\phi_2$ to be orthogonal (perpendicular) to the direction $\\phi_1$.\n",
    "\n",
    "In the example in Figure 6.14, the observations lie in two-dimensional space (since $p = 2$), and so once we have found $\\phi_1$, there is only one possibility for $\\phi_2$, which is shown as a blue dashed line. But in\n",
    "a larger data set with $p > 2$ variables, there are multiple distinct principal components, and they are defined in a similar manner. To find $\\phi_2$, we solve a problem similar to (\\ref{10.3}) with $\\phi_2$ replacing $\\phi_1$, and with the additional constraint that $\\phi_2$ is orthogonal to $\\phi_1$.\n",
    "\n",
    "**Once we have computed the principal components, we can plot them against each other in order to produce low-dimensional views of the data**. For instance, we can plot the score vector $Z_1$ against $Z_2$, $Z_1$ against $Z_3$, $Z_2$ against $Z_3$ , and so forth. Geometrically, this amounts to projecting the original data down onto the subspace spanned by $\\phi_1$, $\\phi_2$, and $\\phi_3$, and plotting the projected points.\n",
    "\n",
    "We illustrate the use of PCA on the **USArrests** data set. For each of the 50 states in the United States, the data set contains the number of arrests per 100, 000 residents for each of three crimes: `Assault`, `Murder`, and `Rape`. We also record `UrbanPop` (the percent of the population in each state living in urban areas).\n",
    "\n",
    "The principal component score vectors have length $n = 50$, and the principal component loading vectors have length $p = 4$. PCA was performed after standardizing each variable to have mean zero and standard deviation one.\n",
    "\n",
    "Figure 10.1 plots the first two principal components of these data. The figure represents both the principal component scores and the loading vectors in a single *biplot* display. The loadings are also given in biplot\n",
    "Table 10.1.\n",
    "\n",
    "|              | PC1       | PC2        |\n",
    "|--------------|-----------|------------|\n",
    "| **Murder**   | 0.5358995 | −0.4181809 |\n",
    "| **Assault**  | 0.5831836 | −0.1879856 |\n",
    "| **UrbanPop** | 0.2781909 | 0.8728062  |\n",
    "| **Rape**     | 0.5434321 | 0.1673186  |\n",
    ">**Table 10.1.** The principal component loading vectors, $\\phi_1$ and $\\phi_2$, for the **USArrests** data.  \n",
    "These are also displayed in Figure 10.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Principal Components for USArrests data](./figures/10.1.png)\n",
    ">**Figure 10.1.** The first two principal components for the **USArrests** data.  \n",
    "The blue state names represent the scores for the first two principal components.  \n",
    "The orange arrows indicate the first two principal component loading vectors (with\n",
    "axes on the top and right).  \n",
    "For example, the loading for `Rape` on the first component is $0.54$, and\n",
    "its loading on the second principal component $0.17$ (the word `Rape` is centered at\n",
    "the point $(0.54, 0.17)$). This figure is known as a biplot, because it displays\n",
    "both the principal component scores and the principal component loadings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Figure 10.1, we see that the first loading vector places approximately equal weight on `Assault`, `Murder`, and `Rape`, with much less weight on `UrbanPop`. Hence this component roughly corresponds to a measure of overall\n",
    "rates of serious crimes. The second loading vector places most of its weight on `UrbanPop` and much less weight on the other three features. Hence, this component roughly corresponds to the level of urbanization of the state.\n",
    "\n",
    "Overall, we see that the crime-related variables (`Murder`, `Assault`, and `Rape`) are located close to each other, and that the `UrbanPop` variable is far from the other three. This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high assault and rape rates—and that the `UrbanPop` variable is less correlated with the other three.\n",
    "\n",
    "Our discussion of the loading vectors suggests that states with large positive scores on the first component, such as **California**, **Nevada** and **Florida**, have high crime rates, while states like **North Dakota**, with negative scores on the first component, have low crime rates.\n",
    "\n",
    "**California** also has a high score on the second component, indicating a high level of urbanization, while the opposite is true for states like **Mississippi**.\n",
    "\n",
    "States close to zero on both components, such as **Indiana**, have approximately average levels of both crime and urbanization.\n",
    "\n",
    "## Another Interpretation of Principal Components\n",
    "The first two principal component loading vectors in a simulated three-dimensional data set are shown in the left-hand panel of Figure 10.2; these two loading vectors span a plane along which the observations have the highest variance.\n",
    "\n",
    "In the previous section, we describe the principal component loading vectors as the directions in feature space along which the data vary the most, and the principal component scores as projections along these directions. However, an alternative interpretation for principal components can also be useful: **principal components provide low-dimensional linear surfaces that are closest to the observations**. We expand upon that interpretation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PCA in three dimensions](./figures/10.2.png)\n",
    ">**Figure 10.2.** Ninety observations simulated in three dimensions.  \n",
    "*Left*: the first two principal component directions span the plane that best fits the data.  \n",
    "It minimizes the sum of squared distances from each point to the plane.  \n",
    "*Right*: the first two principal component score vectors give the coordinates of the projection  \n",
    "of the 90 observations onto the plane. The variance in the plane is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first principal component loading vector** has a very special property: it **is the line in $p$-dimensional space that is *closest* to the $n$ observations (using average squared Euclidean distance as a measure of closeness)**. This interpretation can be seen in the left-hand panel of Figure 6.15; the dashed lines indicate the distance between each observation and the first principal component loading vector. The appeal of this interpretation is clear: we seek a single dimension of the data that lies as close as possible to all of the data points, since such a line will likely provide a good summary of the data.\n",
    "\n",
    "The notion of principal components as the dimensions that are closest to the $n$ observations extends beyond just the first principal component. For instance, **the first two principal components of a data set span the plane that is closest to the $n$ observations**, in terms of average squared Euclidean distance. An example is shown in the left-hand panel of Figure 10.2. The first three principal components of a data set span the three-dimensional hyperplane that is closest to the $n$ observations, and so forth.\n",
    "\n",
    "Using this interpretation, together the first $M$ principal component score vectors and the first $M$ principal component loading vectors provide the best $M$-dimensional approximation (in terms of Euclidean distance) to the $i$th observation $x_{ij}$. This representation can be written\n",
    "\n",
    "\\begin{equation}\\label{10.5}\n",
    "    x_{ij} \\approx \\sum^M_{m=1} z_{im} \\phi_{jm}\n",
    "    \\tag{10.5}\n",
    "\\end{equation}\n",
    "\n",
    "(assuming the original data matrix $\\boldsymbol{X}$ is column-centered). In other words, together the $M$ principal component score vectors and $M$ principal component loading vectors can give a good approximation to the data when $M$ is sufficiently large. When $M = \\min(n-1,p)$, then the representation is exact: $x_{ij} = \\sum^M_{m=1}z_{im}\\phi_{jm}$.\n",
    "\n",
    "## More on PCA\n",
    "### Scaling the Variables\n",
    "We have already mentioned that before PCA is performed, the variables should be centered to have mean zero. Furthermore, **the results obtained when we perform PCA will also depend on whether the variables have been individually scaled** (each multiplied by a different constant). This is in contrast to some other supervised and unsupervised learning techniques, such as linear regression, in which scaling the variables has no effect. (In linear regression, multiplying a variable by a factor of $c$ will simply lead to multiplication of the corresponding coefficient estimate by a factor of $1/c$, and thus will have no substantive effect on the model obtained.)\n",
    "\n",
    "For instance, Figure 10.1 was obtained after scaling each of the variables to have standard deviation one. This is reproduced in the left-hand plot in Figure 10.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scaled vs unscaled](./figures/10.3.png)\n",
    ">**Figure 10.3.** Two principal component biplots for the **USArrests** data.  \n",
    "*Left*: the same as Figure 10.1, with the variables scaled to have unit standard deviations.  \n",
    "*Right*: principal components using unscaled data. `Assault` has by far the largest\n",
    "loading on the first principal component because it has the highest variance among\n",
    "the four variables. In general, scaling the variables to have standard deviation one\n",
    "is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does it matter that we scaled the variables? In these data, the variables are measured in different units; `Murder`, `Rape`, and `Assault` are reported as the number of occurrences per $100,000$ people, and `UrbanPop` is the percentage of the state’s population that lives in an urban area. These four variables have variance $18.97, 87.73, 6945.16,$ and $209.5$, respectively. Consequently, if we perform PCA on the unscaled variables, then the first principal component loading vector will have a very large loading for `Assault`, since that variable has by far the highest variance.\n",
    "\n",
    "The right-hand plot in Figure 10.3 displays the first two principal components for the **USArrests** data set, without scaling the variables to have standard deviation one. As predicted, **the first principal component loading vector places almost all of its weight on `Assault`, while the second principal component loading vector places almost all of its weight on `UrpanPop`**. Comparing this to the left-hand plot, we see that scaling does indeed have a substantial effect on the results obtained.\n",
    "\n",
    "However, this result is simply a consequence of the scales on which the variables were measured. For instance, **if `Assault` were measured in units of the number of occurrences per 100 people** (rather than number of occurrences per 100,000 people), **then this would amount to dividing all of the elements of that variable by 1,000**. Then the variance of the variable would be tiny, and so the first principal component loading vector would have a very small value for that variable. **Because it is undesirable for the principal components obtained to depend on an arbitrary choice of scaling, we typically scale each variable to have standard deviation one before we perform PCA**.\n",
    "\n",
    "In certain settings, however, the variables may be measured in the same units. In this case, we might not wish to scale the variables to have standard deviation one before performing PCA. For instance, suppose that the variables in a given data set correspond to expression levels for $p$ genes. Then since expression is measured in the same “units” for each gene, we might choose not to scale the genes to each have standard deviation one.\n",
    "\n",
    "### Uniqueness of the Principal Components\n",
    "Each principal component loading vector is unique, up to a sign flip. This means that two different software packages will yield the same principal component loading vectors, although the signs of those loading vectors may differ. The signs may differ because each principal component loading vector specifies a direction in $p$-dimensional space: flipping the sign has no effect as the direction does not change. (Consider Figure 6.14—the principal component loading vector is a line that extends in either direction, and flipping its sign would have no effect.)\n",
    "\n",
    "Similarly, the score vectors are unique up to a sign flip, since the variance of Z is the same as the variance of $−Z$. It is worth noting that when we use (\\ref{10.5}) to approximate $x_{ij}$ we multiply zim by $\\phi_{jm}$. Hence, if the sign is flipped on both the loading and score vectors, the final product of the two quantities is unchanged.\n",
    "\n",
    "### The Proportion of Variance Explained\n",
    "In Figure 10.2, we performed PCA on a three-dimensional data set (left-hand panel) and projected the data onto the first two principal component loading vectors in order to obtain a two-dimensional view of the data (i.e. the principal component score vectors; right-hand panel).\n",
    "\n",
    "We see that this two-dimensional representation of the three-dimensional data does successfully capture the major pattern in the data: the orange, green, and cyan observations that are near each other in three-dimensional space remain nearby in the two-dimensional representation. Similarly, we have seen on the **USArrests** data set that we can summarize the 50 observations and 4 variables using just the first two principal component score vectors and the first two principal component loading vectors.\n",
    "\n",
    "We can now ask a natural question: **how much of the information in a given data set is lost by projecting the observations onto the first few principal components**? That is, how much of the variance in the data is not\n",
    "contained in the first few principal components? More generally, we are interested in knowing the **proportion of variance explained** (PVE) by each proportion principal component. The total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as\n",
    "\n",
    "\\begin{equation}\\label{10.6}\n",
    "    \\sum^p_{j=1} \\text{Var}(X_j) = \\sum^p_{j=1} \\frac{1}{n} \\sum^n_{i=1} x^2_{ij},\n",
    "    \\tag{10.6}\n",
    "\\end{equation}\n",
    "\n",
    "and the variance explained by the $m$th principal component is\n",
    "\n",
    "\\begin{equation}\\label{10.7}\n",
    "    \\frac{1}{n} \\sum^n_{i=1}z^2_{im} = \\frac{1}{n} \\sum^n_{i=1} \\left( \\sum^p_{j=1}\\phi_{jm}x_{ij} \\right)^2.\n",
    "    \\tag{10.7}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, the PVE of the $m$th principal component is given by\n",
    "\n",
    "\\begin{equation}\\label{10.8}\n",
    "    \\frac{ \\sum^n_{i=1} \\left(\\sum^p_{j=1} \\phi_{jm}x_{ij}\\right)^2 }{ \\sum^p_{j=1} \\sum^n_{i=1} x^2_{ij} }\n",
    "    \\tag{10.8}\n",
    "\\end{equation}\n",
    "\n",
    "The PVE of each principal component is a positive quantity. In order to compute the cumulative PVE of the first $M$ principal components, we can simply sum (\\ref{10.8}) over each of the first $M$ PVEs. In total, there are $\\min(n − 1, p)$ principal components, and their PVEs sum to one.\n",
    "\n",
    "In the **USArrests** data, the first principal component explains $62.0 \\%$ of the variance in the data, and the next principal component explains $24.7 \\%$ of the variance. Together, the first two principal components explain almost $87 \\%$ of the variance in the data, and the last two principal components explain only $13 \\%$ of the variance. This means that Figure 10.1 provides a pretty accurate summary of the data using just two dimensions. The PVE of each principal component, as well as the cumulative PVE, is shown in Figure 10.4. The left-hand panel is known as a *scree plot*, and will be discussed next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scree Plot and Variance Explained for USArrests](./figures/10.4.png)\n",
    ">**Figure 10.4.** *Left*: a scree plot depicting the proportion of variance explained\n",
    "by each of the four principal components in the **USArrests** data.  \n",
    "*Right*: the cumulative proportion of variance explained by the four principal components in the\n",
    "USArrests data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding How Many Principal Components to Use\n",
    "In general, a $n \\times p$ data matrix $\\boldsymbol{X}$ has $\\min(n − 1, p)$ distinct principal components. However, we usually are not interested in all of them; rather, we would like to use just the first few principal components in order to visualize or interpret the data. In fact, *we would like to use the smallest number of principal components required to get a good understanding of the data*. How many principal components are needed? Unfortunately, there is no single (or simple!) answer to this question.\n",
    "\n",
    "We typically decide on the number of principal components required to visualize the data by examining a scree plot, such as the one shown in the left-hand panel of Figure 10.4. **We choose the smallest number of principal components that are required in order to explain a sizable amount of the variation in the data. This is done by eyeballing the scree plot, and looking for a point at which the proportion of variance explained by each subsequent principal component drops off**. This is often referred to as an *elbow* in the scree plot.\n",
    "\n",
    "For instance, by inspection of Figure 10.4, one might conclude that a fair amount of variance is explained by the first two principal components, and that there is an elbow after the second component. After all, the third principal component explains less than ten percent of the variance in the data, and the fourth principal component explains less than half that and so is essentially worthless.\n",
    "\n",
    "However, this type of visual analysis is inherently *ad hoc*. Unfortunately, there is no well-accepted objective way to decide how many principal components are *enough*. In fact, *the question of how many principal components are enough is inherently ill-defined, and will depend on the specific area of application and the specific data set*.\n",
    "\n",
    "In practice, we tend to look at the first few principal components in order to find interesting patterns in the data. **If no interesting patterns are found in the first few principal components, then further principal components are unlikely to be of interest**. Conversely, **if the first few principal components are interesting, then we typically continue to look at subsequent principal components until no further interesting patterns are found**. This is admittedly a subjective approach, and is reflective of the fact that PCA is generally used as a tool for exploratory data analysis.\n",
    "\n",
    "On the other hand, if we compute principal components for use in a *supervised analysis*, such as the principal components regression presented in Section 6.3.1, then there is a simple and objective way to determine how\n",
    "many principal components to use: we can treat the number of principal component score vectors to be used in the regression as a tuning parameter to be selected via cross-validation or a related approach.\n",
    "\n",
    "The comparative simplicity of selecting the number of principal components for a supervised analysis is one manifestation of the fact that supervised analyses tend to be more clearly defined and more objectively evaluated than unsupervised analyses.\n",
    "\n",
    "## Other Uses for Principal Components\n",
    "We saw in Section 6.3.1 that we can perform regression using the principal component score vectors as features. In fact, many statistical techniques, such as regression, classification, and clustering, can be easily adapted to use the $n \\times M$ matrix whose columns are the first $M \\ll p$ principal component score vectors, rather than using the full $n \\times p$ data matrix. This can lead to less noisy results, since it is often the case that the signal (as opposed to the noise) in a data set is concentrated in its first few principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Methods\n",
    "**Clustering** refers to a very broad set of techniques for finding *subgroups*, or *clusters*, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. Of course, to make this concrete, we must define what it means for two or more observations to be *similar* or *different*. Indeed, this is often a domain-specific consideration that must be made based on knowledge of the data being studied.\n",
    "\n",
    "For instance, suppose that we have a set of $n$ observations, each with $p$ features. The $n$ observations could correspond to tissue samples for patients with breast cancer, and the $p$ features could correspond to measurements collected for each tissue sample; these could be clinical measurements, such as tumor stage or grade, or they could be gene expression measurements. We may have a reason to believe that there is some heterogeneity among the $n$ tissue samples; for instance, perhaps there are a few different unknown subtypes of breast cancer. Clustering could be used to find these subgroups. This is an unsupervised problem because we are trying to discover structure—in this case, distinct clusters—on the basis of a data set. The goal in supervised problems, on the other hand, is to try to predict some outcome vector such as survival time or response to drug treatment.\n",
    "\n",
    "**Both clustering and PCA seek to simplify the data via a small number of summaries, but their mechanisms are different**:\n",
    "- PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance;\n",
    "- Clustering looks to find homogeneous subgroups among the observations.\n",
    "\n",
    "Another application of clustering arises in marketing. We may have access to a large number of measurements (e.g. median household income, occupation, distance from nearest urban area, and so forth) for a large number of people. Our goal is to perform market segmentation by identifying subgroups of people who might be more receptive to a particular form of advertising, or more likely to purchase a particular product. The task of performing market segmentation amounts to clustering the people in the data set.\n",
    "\n",
    "Since clustering is popular in many fields, there exist a great number of clustering methods. In this section we focus on perhaps the two best-known clustering approaches: K-means clustering and hierarchical clustering. **In K-means clustering, we seek to partition the observations into a pre-specified number of clusters**. On the other hand, **in hierarchical clustering**, we do not know in advance how many clusters we want; in fact, **we end up with a tree-like visual representation of the observations**, called a dendrogram, **that allows us to view at once the clusterings obtained for each possible number of clusters, from 1 to $n$**. There are advantages and disadvantages to each of these clustering approaches, which we highlight in this chapter.\n",
    "\n",
    "In general, **we can cluster observations on the basis of the features in order to identify subgroups among the observations, or we can cluster features on the basis of the observations in order to discover subgroups among\n",
    "the features**. In what follows, for simplicity we will discuss clustering observations on the basis of the features, though the converse can be performed by simply transposing the data matrix.\n",
    "\n",
    "## K-Means Clustering\n",
    "K-means clustering is a simple and elegant approach for partitioning a data set into K distinct, non-overlapping clusters. To perform K-means clustering, we must first specify the desired number of clusters K; then the K-means algorithm will assign each observation to exactly one of the K clusters. Figure 10.5 shows the results obtained from performing K-means clustering on a simulated example consisting of 150 observations in two dimensions, using three different values of K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![K-means clustering](./figures/10.5.png)\n",
    ">**Figure 10.5.** A simulated data set with 150 observations in two-dimensional\n",
    "space. Panels show the results of applying K-means clustering with different\n",
    "values of K, the number of clusters. The color of each observation indicates the\n",
    "cluster to which it was assigned using the K-means clustering algorithm. Note that\n",
    "there is no ordering of the clusters, so the cluster coloring is arbitrary. These\n",
    "cluster labels were not used in clustering; instead, they are the outputs of the\n",
    "clustering procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-means clustering procedure results from a simple and intuitive mathematical problem. We begin by defining some notation. Let $C_1, \\ldots, C_K$ denote sets containing the indices of the observations in each cluster. These sets satisfy two properties:\n",
    "1. $\\;C_1 \\cup C_2 \\cup \\ldots \\cup C_K = \\{1, \\ldots, n\\}$. In other words, each observation belongs to at least one of the K clusters.\n",
    "2. $\\;C_k \\cap C_{k'} = \\emptyset$ for all $k \\ne k'$. In other words, the clusters are non-overlapping: no observation belongs to more than one cluster.\n",
    "\n",
    "For instance, if the ith observation is in the kth cluster, then $i \\in C_k$. The idea behind K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. The within-cluster variation for cluster $C_k$ is a measure $W(C_k)$ of the amount by which the observations within a cluster differ from each other. Hence we want to solve the problem\n",
    "\n",
    "\\begin{equation}\\label{10.9}\n",
    "    { \\text{minimize } \\atop C_1, \\ldots, C_K } \\left\\{ \\sum^K_{k=1} W(C_k) \\right\\}\n",
    "    \\tag{10.9}\n",
    "\\end{equation}\n",
    "\n",
    "In words, this formula says that we want to partition the observations into K clusters such that the total within-cluster variation, summed over all K clusters, is as small as possible.\n",
    "\n",
    "Solving (\\ref{10.9}) seems like a reasonable idea, but in order to make it actionable we need to define the within-cluster variation. There are many possible ways to define this concept, but by far the most common choice involves squared Euclidean distance. That is, we define\n",
    "\n",
    "\\begin{equation}\\label{10.10}\n",
    "    W(C_k) = \\frac{1}{|C_k|} \\sum_{i, i' \\in C_k} \\sum^p_{j=1} (x_{ij} - x_{i'j})^2,\n",
    "    \\tag{10.10}\n",
    "\\end{equation}\n",
    "\n",
    "where $|C_k|$ denotes the number of observations in the $k$th cluster. In other words, the within-cluster variation for the $k$th cluster is the sum of all of the pairwise squared Euclidean distances between the observations in the $k$th cluster, divided by the total number of observations in the $k$th cluster.\n",
    "\n",
    "Combining (\\ref{10.9}) and (\\ref{10.10}) gives the optimization problem that defines K-means clustering,\n",
    "\n",
    "\\begin{equation}\\label{10.11}\n",
    "    { \\text{minimize } \\atop C_1, \\ldots, C_K } \\left\\{ \\sum^K_{k=1} \\frac{1}{|C_k|} \\sum_{i, i' \\in C_k} \\sum^p_{j=1} (x_{ij} - x_{i'j})^2 \\right\\}\n",
    "    \\tag{10.11}\n",
    "\\end{equation}\n",
    "\n",
    "Now, we would like to find an algorithm to solve (\\ref{10.11})—that is, a method to partition the observations into $K$ clusters such that the objective of (\\ref{10.11}) is minimized. This is in fact a very difficult problem to solve precisely, since there are almost $K^n$ ways to partition $n$ observations into $K$ clusters. This is a huge number unless $K$ and $n$ are tiny!\n",
    "\n",
    "Fortunately, a very simple algorithm can be shown to provide a local optimum to the K-means optimization problem (\\ref{10.11}). This approach is laid out in Algorithm 10.1.\n",
    "\n",
    "**Algorithm 10.1 -** *K-Means Clustering*\n",
    "\n",
    ">1. Randomly assign a number, from $1$ to $K$, to each of the observations. These serve as initial cluster assignments for the observations.\n",
    ">2. Iterate until the cluster assignments stop changing:\n",
    ">\n",
    ">    (a) For each of the $K$ clusters, compute the cluster centroid. The $k$th cluster centroid is the vector of the $p$ feature means for the observations in the $k$th cluster.\n",
    ">\n",
    ">    (b) Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).\n",
    "\n",
    "Algorithm 10.1 is guaranteed to decrease the value of the objective (\\ref{10.11}) at each step. To understand why, the following identity is illuminating:\n",
    "\n",
    "\\begin{equation}\\label{10.12}\n",
    "    \\frac{1}{|C_k|} \\sum_{i,i' \\in C_k} \\sum^p_{j=1} (x_{ij} - x_{i'j})^2 = 2 \\sum_{i \\in C_k} \\sum^p_{j=1} (x_{ij} - \\bar{x}_{kj})^2,\n",
    "    \\tag{10.12}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\bar{x}_{kj} = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x_{ij}$ is the mean for feature $j$ in cluster $C_k$.\n",
    "\n",
    "In *Step 2(a)* the cluster means for each feature are the constants that minimize the sum-of-squared deviations, and in *Step 2(b)*, reallocating the observations can only improve (\\ref{10.12}). This means that as the algorithm is run, the clustering obtained will continually improve until the result no longer changes; the objective of (\\ref{10.11}) will never increase. When the result no longer changes, a **local optimum** has been reached.\n",
    "\n",
    "Figure 10.6 shows the progression of the algorithm on the toy example from Figure 10.5. K-means clustering derives its name from the fact that in *Step 2(a)*, the cluster centroids are computed as the mean of the observations assigned to each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![K-means algorithm](./figures/10.6.png)\n",
    ">**Figure 10.6.** The progress of the K-means algorithm on the example of Figure 10.5 with K=3.  \n",
    "*Top left*: the observations are shown.  \n",
    "*Top center*: in Step 1 of the algorithm, each observation is randomly assigned to a cluster.  \n",
    "*Top right*: in Step 2(a), the cluster centroids are computed. These are shown\n",
    "as large colored disks. Initially the centroids are almost completely overlapping\n",
    "because the initial cluster assignments were chosen at random.  \n",
    "*Bottom left*: in Step 2(b), each observation is assigned to the nearest centroid.  \n",
    "*Bottom center*: Step 2(a) is once again performed, leading to new cluster centroids.  \n",
    "*Bottom right*: the results obtained after ten iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the K-means algorithm finds a local rather than a global optimum, the results obtained will depend on the initial (random) cluster assignment of each observation in Step 1 of Algorithm 10.1. **For this reason, it is important to run the algorithm multiple times from different random initial configurations**. Then one selects the best solution, i.e. that for which the objective (\\ref{10.11}) is smallest.\n",
    "\n",
    "Figure 10.7 shows the local optima obtained by running K-means clustering six times using six different initial cluster assignments, using the toy data from Figure 10.5. In this case, the best clustering is the one with an objective value of 235.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![K-means six times](./figures/10.7.png)\n",
    ">**Figure 10.7.** K-means clustering performed six times on the data from\n",
    "Figure 10.5 with K = 3, each time with a different random assignment of the\n",
    "observations in Step 1 of the K-means algorithm. Above each plot is the value of\n",
    "the objective (\\ref{10.11}). Three different local optima were obtained, one of which\n",
    "resulted in a smaller value of the objective and provides better separation between\n",
    "the clusters. Those labeled in red all achieved the same best solution, with an\n",
    "objective value of 235.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, to perform K-means clustering, we must decide how many clusters we expect in the data. The problem of selecting K is far from simple. This issue, along with other practical considerations that arise in performing K-means clustering, is addressed in [Section 10.3.3](#Practical-Issues-in-Clustering).\n",
    "\n",
    "## Hierarchical Clustering\n",
    "One potential disadvantage of K-means clustering is that it requires us to pre-specify the number of clusters K. **Hierarchical clustering** is an alternative approach which does not require that we commit to a particular choice of K. Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram.\n",
    "\n",
    "In this section, we describe **bottom-up** or **agglomerative clustering**. This is the most common type of hierarchical clustering, and refers to the fact that **a dendrogram** (generally depicted as an upside-down tree; see Figure 10.9) **is built starting from the leaves and combining clusters up to the trunk**. We will begin with a discussion of how to interpret a dendrogram and then discuss how hierarchical clustering is actually performed—that is, how the dendrogram is built.\n",
    "\n",
    "### Interpreting a Dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![45 generated observations](./figures/10.8.png)\n",
    ">**Figure 10.8.** Forty-five observations generated in two-dimensional space. In\n",
    "reality there are three distinct classes, shown in separate colors. However, we will\n",
    "treat these class labels as unknown and will seek to cluster the observations in\n",
    "order to discover the classes from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with the simulated data set shown in Figure 10.8, consisting of 45 observations in two-dimensional space. The data were generated from a three-class model; the true class labels for each observation are shown in distinct colors. However, suppose that the data were observed without the class labels, and that we wanted to perform hierarchical clustering of the data. Hierarchical clustering (with complete linkage, to be discussed later) yields the result shown in the left-hand panel of Figure 10.9. How can we interpret this dendrogram?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dendrograms](./figures/10.9.png)\n",
    ">**Figure 10.9.** *Left*: dendrogram obtained from hierarchically clustering the data\n",
    "from Figure 10.8 with complete linkage and Euclidean distance.  \n",
    "*Center*: the dendrogram from the left-hand panel, cut at a height of nine (indicated by\n",
    "the dashed line). This cut results in two distinct clusters, shown in different colors.  \n",
    "*Right*: the dendrogram from the left-hand panel, now cut at a height of five. This cut\n",
    "results in three distinct clusters, shown in different colors. Note that the colors\n",
    "were not used in clustering, but are simply used for display purposes in this figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left-hand panel of Figure 10.9, each leaf of the dendrogram represents one of the 45 observations in Figure 10.8. However, *as we move up the tree, some leaves begin to fuse into branches. These correspond to observations that are similar to each other. As we move higher up the tree, branches themselves fuse, either with leaves or other branches. The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other. On the other hand, observations that fuse later (near the top of the tree) can be quite different*.\n",
    "\n",
    "**In fact, this statement can be made precise: for any two observations, we can look for the point in the tree where branches containing those two observations are first fused. The height of this fusion, as measured on the vertical axis, indicates how different the two observations are**. Thus, observations that fuse at the very\n",
    "bottom of the tree are quite similar to each other, whereas observations that fuse close to the top of the tree will tend to be quite different.\n",
    "\n",
    "This highlights a very important point in interpreting dendrograms that is often misunderstood. Consider the left-hand panel of Figure 10.10, which shows a simple dendrogram obtained from hierarchically clustering nine observations. One can see that observations 5 and 7 are quite similar to each other, since they fuse at the lowest point on the dendrogram. Observations 1 and 6 are also quite similar to each other.\n",
    "\n",
    "However, it is tempting but incorrect to conclude from the figure that observations 9 and 2 are quite similar to each other on the basis that they are located near each other on the dendrogram. **In fact, based on the information contained in the dendrogram, observation 9 is no more similar to observation 2 than it is to observations 8, 5, and 7**. This is because observations 2, 8, 5, and 7 all fuse with observation 9 at the same height, approximately 1.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dendrogram vs Raw data](./figures/10.10.png)\n",
    ">**Figure 10.10.** An illustration of how to properly interpret a dendrogram with\n",
    "nine observations in two-dimensional space.  \n",
    "*Left*: a dendrogram generated using Euclidean distance and complete linkage.  \n",
    "*Right*: the raw data used to generate the dendrogram can be used to\n",
    "confirm that indeed, observation 9 is no more similar to observation 2 than it is\n",
    "to observations 8, 5, and 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put it mathematically, there are $2^{n−1}$ possible reorderings of the dendrogram, where $n$ is the number of leaves. This is because at each of the $n − 1$ points where fusions occur, the positions of the two fused branches could be swapped without affecting the meaning of the dendrogram. Therefore, *we cannot draw conclusions about the similarity of two observations based on their proximity along the horizontal axis*. Rather, **we draw conclusions about the similarity of two observations based on the location on the *vertical axis* where branches containing those two observations first are fused**.\n",
    "\n",
    "Now that we understand how to interpret the left-hand panel of Figure 10.9, we can move on to the issue of identifying clusters on the basis of a dendrogram. In order to do this, we make a horizontal cut across the dendrogram, as shown in the center and right-hand panels of Figure 10.9.\n",
    "\n",
    "The distinct sets of observations beneath the cut can be interpreted as clusters. In the center panel of Figure 10.9, cutting the dendrogram at a height of nine results in two clusters, shown in distinct colors. In the right-hand panel, cutting the dendrogram at a height of five results in three clusters.\n",
    "\n",
    "Further cuts can be made as one descends the dendrogram in order to obtain any number of clusters, between 1 (corresponding to no cut) and $n$ (corresponding to a cut at height 0, so that each observation is in its own cluster). **In other words, the height of the cut to the dendrogram serves the same role as the $K$ in K-means clustering: it controls the number of clusters obtained**.\n",
    "\n",
    "Figure 10.9 therefore highlights a very attractive aspect of hierarchical clustering: **one single dendrogram can be used to obtain any number of clusters**. In practice, people often look at the dendrogram and select by eye\n",
    "a sensible number of clusters, based on the heights of the fusion and the number of clusters desired. In the case of Figure 10.9, one might choose to select either two or three clusters. However, often the choice of where to cut the dendrogram is not so clear.\n",
    "\n",
    "The term hierarchical refers to the fact that clusters obtained by cutting the dendrogram at a given height are necessarily nested within the clusters obtained by cutting the dendrogram at any greater height. However, on an arbitrary data set, this assumption of hierarchical structure might be unrealistic.\n",
    "\n",
    "For instance, suppose that our observations correspond to a group of people with a 50–50 split of males and females, evenly split among Americans, Japanese, and French. *We can imagine a scenario in which the best division into two groups might split these people by gender, and the best division into three groups might split them by nationality*.\n",
    "\n",
    "In this case, the true clusters are not nested, in the sense that the best division into three groups does not result from taking the best division into two groups and splitting up one of those groups. Consequently, this situation could not be well-represented by hierarchical clustering. Due to situations such as this one, hierarchical clustering can sometimes yield worse (i.e. less accurate) results than K-means clustering for a given number of clusters\n",
    "\n",
    "### The Hierarchical Clustering Algorithm\n",
    "The hierarchical clustering dendrogram is obtained via an extremely simple algorithm. We begin by defining some sort of dissimilarity measure between each pair of observations. Most often, Euclidean distance is used; we will\n",
    "discuss the choice of dissimilarity measure later in this chapter. The algorithm proceeds iteratively.\n",
    "\n",
    "Starting out at the bottom of the dendrogram, each of the $n$ observations is treated as its own cluster. The two clusters that are most similar to each other are then fused so that there now are $n − 1$ clusters. Next the two clusters that are most similar to each other are fused again, so that there now are $n − 2$ clusters. The algorithm proceeds in this fashion until all of the observations belong to one single cluster, and the dendrogram is complete. Figure 10.11 depicts the first few steps of the algorithm, for the data from Figure 10.9. To summarize, the hierarchical clustering algorithm is given in Algorithm 10.2.\n",
    "\n",
    "\n",
    "**Algorithm 10.2 -** *Hierarchical Clustering*\n",
    "\n",
    ">1. Begin with n observations $n$ and a measure (such as Euclidean distance) of all the ${n \\choose 2} = n(n − 1)/2$ pairwise dissimilarities. Treat each observation as its own cluster.\n",
    ">2. For $i = n, n-1, n-2, \\ldots, 2$:\n",
    ">\n",
    ">    (a) Examine all pairwise inter-cluster dissimilarities among the $i$ clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.\n",
    ">\n",
    ">    (b) Compute the new pairwise inter-cluster dissimilarities among the $i − 1$ remaining clusters.\n",
    "\n",
    "This algorithm seems simple enough, but one issue has not been addressed. Consider the bottom right panel in Figure 10.11. How did we determine that the cluster $\\{5, 7\\}$ should be fused with the cluster $\\{8\\}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hierarchical clustering algorithm](./figures/10.11.png)\n",
    ">**Figure 10.11.** An illustration of the first few steps of the hierarchical\n",
    "clustering algorithm, using the data from Figure 10.10, with complete linkage\n",
    "and Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a concept of the dissimilarity between pairs of observations, but how do we define the dissimilarity between two clusters if one or both of the clusters contains multiple observations? The concept of dissimilarity between a pair of observations needs to be extended to a pair of *groups of observations*. \n",
    "\n",
    "This extension is achieved by developing the notion of linkage, which defines the dissimilarity between two groups of observations. The four most common types of linkage—complete, average, single, and centroid—are briefly described in Table 10.2.\n",
    "\n",
    "| Linkage | Description |\n",
    "|-|:-:|\n",
    "| Complete | Maximal intercluster dissimilarity.<br>Compute all pairwise dissimilarities between<br>the observations in cluster A and the observations<br>in cluster B, and record the largest of these dissimilarities. |\n",
    "| Single | Minimal intercluster dissimilarity.<br>Compute all pairwise dissimilarities between <br>the observations in cluster A and the observations<br>in cluster B, and record the smallest of these dissimilarities.<br>Single linkage can result in extended, trailing clusters in which<br>single observations are fused one-at-a-time |\n",
    "| Average | Mean intercluster dissimilarity.<br>Compute all pairwise dissimilarities between<br>the observations in cluster A and the observations<br>in cluster B, and record the average of these dissimilarities. |\n",
    "| Centroid | Dissimilarity between the centroid for cluster A<br> (a mean vector of length $p$) and the centroid for cluster B.<br>Centroid linkage can result in undesirable inversions. |\n",
    ">**Table 10.1.** A summary of the four most commonly-used types of linkage in hierarchical clustering.\n",
    "\n",
    "Average, complete, and single linkage are most popular among statisticians. Average and complete linkage are generally preferred over single linkage, as they tend to yield more balanced dendrograms. Centroid linkage is often used in genomics, but suffers from a major drawback in that an inversion can occur, whereby two clusters are fused at a height below either of the individual clusters in the dendrogram. This can lead to difficulties in visualization as well as in interpretation of the dendrogram.\n",
    "\n",
    "The dissimilarities computed in *Step 2(b)* of the hierarchical clustering algorithm will depend on the type of linkage used, as well as on the choice of dissimilarity measure. Hence, the resulting dendrogram typically depends quite strongly on the type of linkage used, as is shown in Figure 10.12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Average, complete, single linkage](./figures/10.12.png)\n",
    ">**Figure 10.12.** Average, complete, and single linkage applied to an example\n",
    "data set. Average and complete linkage tend to yield more balanced clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of Dissimilarity Measure\n",
    "Thus far, the examples in this chapter have used Euclidean distance as the dissimilarity measure. But sometimes other dissimilarity measures might be preferred. For example, **correlation-based distance** considers two observations to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance.\n",
    "\n",
    "This is an unusual use of correlation, which is normally computed between variables; here it is computed between the observation profiles for each pair of observations. Figure 10.13 illustrates the difference between Euclidean\n",
    "and correlation-based distance. Correlation-based distance focuses on the shapes of observation profiles rather than their magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Correlation-based distance](./figures/10.13.png)\n",
    ">**Figure 10.13.** Three observations with measurements on 20 variables are\n",
    "shown. Observations 1 and 3 have similar values for each variable and so there\n",
    "is a small Euclidean distance between them. But they are very weakly correlated,\n",
    "so they have a large correlation-based distance. On the other hand, observations\n",
    "1 and 2 have quite different values for each variable, and so there is a large\n",
    "Euclidean distance between them. But they are highly correlated, so there is a\n",
    "small correlation-based distance between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of dissimilarity measure is very important, as it has a strong effect on the resulting dendrogram. In general, careful attention should be paid to the type of data being clustered and the scientific question at hand. These considerations should determine what type of dissimilarity measure is used for hierarchical clustering.\n",
    "\n",
    "For instance, consider an online retailer interested in clustering shoppers based on their past shopping histories. The goal is to identify subgroups of similar shoppers, so that shoppers within each subgroup can be shown items and advertisements that are particularly likely to interest them.\n",
    "\n",
    "Suppose the data takes the form of a matrix where the rows are the shoppers and the columns are the items available for purchase; the elements of the data matrix indicate the number of times a given shopper has purchased a given item (i.e. a 0 if the shopper has never purchased this item, a 1 if the shopper has purchased it once, etc.) What type of dissimilarity measure should be used to cluster the shoppers?\n",
    "\n",
    "**If Euclidean distance is used, then shoppers who have bought very few items overall** (i.e. infrequent users of\n",
    "the online shopping site) **will be clustered together**. This may not be desirable. On the other hand, **if correlation-based distance is used, then shoppers with similar preferences** (e.g. shoppers who have bought items A and B but never items C or D) **will be clustered together, even if some shoppers with these preferences are higher-volume shoppers than others**. Therefore, for this application, correlation-based distance may be a better choice.\n",
    "\n",
    "In addition to carefully selecting the dissimilarity measure used, one must also consider whether or not the variables should be scaled to have standard deviation one before the dissimilarity between the observations is\n",
    "computed. To illustrate this point, we continue with the online shopping example just described.\n",
    "\n",
    "Some items may be purchased more frequently than others; for instance, a shopper might buy ten pairs of socks a year, but a computer very rarely. High-frequency purchases like socks therefore tend to have a much larger effect on the inter-shopper dissimilarities, and hence on the clustering ultimately obtained, than rare purchases like computers. This may not be desirable. If the variables are scaled to have standard deviation one before the inter-observation dissimilarities are computed, then each variable will in effect be given equal importance in the hierarchical clustering performed.\n",
    "\n",
    "We might also want to scale the variables to have standard deviation one if they are measured on different scales; otherwise, the choice of units (e.g. centimeters versus kilometers) for a particular variable will greatly affect the dissimilarity measure obtained. **Whether or not it is a good decision to scale the variables before computing the dissimilarity measure depends on the application at hand**. An example is shown in Figure 10.14. **We note that the issue of whether or not to scale the variables before performing clustering applies to K-means clustering as well**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Online retailer example](./figures/10.14.png)\n",
    ">**Figure 10.14.** An eclectic online retailer sells two items: socks and computers.  \n",
    "*Left*: the number of pairs of socks, and computers, purchased by eight online\n",
    "shoppers is displayed. Each shopper is shown in a different color. If inter-observation\n",
    "dissimilarities are computed using Euclidean distance on the raw variables, then\n",
    "the number of socks purchased by an individual will drive the dissimilarities\n",
    "obtained, and the number of computers purchased will have little effect. This might be\n",
    "undesirable, since (1) computers are more expensive than socks and so the online\n",
    "retailer may be more interested in encouraging shoppers to buy computers than\n",
    "socks, and (2) a large difference in the number of socks purchased by two shoppers\n",
    "may be less informative about the shoppers’ overall shopping preferences than a\n",
    "small difference in the number of computers purchased.  \n",
    "*Center*: the same data is shown, after scaling each variable by its standard\n",
    "deviation. Now the number of computers purchased will have a much greater effect on\n",
    "the inter-observation dissimilarities obtained.  \n",
    "*Right*: the same data are displayed, but now the y-axis represents the number of\n",
    "dollars spent by each online shopper on socks and on computers. Since computers are\n",
    "much more expensive than socks, now computer purchase history will drive the\n",
    "inter-observation dissimilarities obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Issues in Clustering\n",
    "Clustering can be a very useful tool for data analysis in the unsupervised setting. However, there are a number of issues that arise in performing clustering. We describe some of these issues here.\n",
    "\n",
    "### Small Decisions with Big Consequences\n",
    "In order to perform clustering, some decisions must be made.\n",
    "- Should the observations or features first be standardized in some way? For instance, maybe the variables should be centered to have mean zero and scaled to have standard deviation one.\n",
    "-  In the case of hierarchical clustering,\n",
    "    - What dissimilarity measure should be used?\n",
    "    - What type of linkage should be used?\n",
    "    - Where should we cut the dendrogram in order to obtain clusters?\n",
    "- In the case of K-means clustering, how many clusters should we look for in the data?\n",
    "\n",
    "Each of these decisions can have a strong impact on the results obtained. **In practice, we try several different choices, and look for the one with the most useful or interpretable solution**. With these methods, there is no single right answer—any solution that exposes some interesting aspects of the data should be considered.\n",
    "\n",
    "### Validating the Clusters Obtained\n",
    "**Any time clustering is performed on a data set we will find clusters**. But we really want to know whether the clusters that have been found represent true subgroups in the data, or whether they are simply a result of clustering the noise.\n",
    "\n",
    "For instance, if we were to obtain an independent set of observations, then would those observations also display the same set of clusters? This is a hard question to answer. There exist a number of techniques for assigning a p-value to a cluster in order to assess whether there is more evidence for the cluster than one would expect due to chance. However, there has been no consensus on a single best approach.\n",
    "\n",
    "### Other Considerations in Clustering\n",
    "**Both K-means and hierarchical clustering will assign each observation to a cluster**. However, sometimes this might not be appropriate.\n",
    "\n",
    "For instance, suppose that most of the observations truly belong to a small number of (unknown) subgroups, and a small subset of the observations are quite different from each other and from all other observations. Then **since K-means and hierarchical clustering force every observation into a cluster, the clusters found may be heavily distorted due to the presence of outliers that do not belong to any cluster**. Mixture models are an attractive approach for accommodating the presence of such outliers. These amount to a soft version of K-means clustering, and are described in Hastie et al. (2009).\n",
    "\n",
    "In addition, **clustering methods generally are not very robust to perturbations to the data**. For instance, suppose that we cluster $n$ observations, and then cluster the observations again after removing a subset of the $n$ observations at random. One would hope that the two sets of clusters obtained would be quite similar, but often this is not the case!\n",
    "\n",
    "### A Tempered Approach to Interpreting the Results of Clustering\n",
    "We have described some of the issues associated with clustering. However, clustering can be a very useful and valid statistical tool if used properly. We mentioned that small decisions in how clustering is performed, such as how the data are standardized and what type of linkage is used, can have a large effect on the results. Therefore, **we recommend performing clustering with different choices of these parameters, and looking at the full set of results in order to see what patterns consistently emerge**.\n",
    "\n",
    "Since clustering can be non-robust, **we recommend clustering *subsets* of the data in order to get a sense of the robustness of the clusters obtained**. Most importantly, we must be careful about how the results of a clustering analysis are reported. These results should not be taken as the absolute truth about a data set. Rather, they should constitute a starting point for the development of a scientific hypothesis and further study, preferably on an independent data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# End Chapter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
